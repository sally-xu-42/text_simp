{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Access local files"
      ],
      "metadata": {
        "id": "87KBZt_WogTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "0HHPYriAolGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "710f0980-bd32-4963-88f1-2651b72b1986"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/playground/allennlp/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHLv3DCGqG5V",
        "outputId": "d16e073f-8963-4d29-e05d-4dd618b9dba2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/playground/allennlp/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HbkkppsqXkK",
        "outputId": "240ae9a2-a471-4c25-cd48-a21dad7a1cc2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all.tsv\t\t    glove.6B.zip  seq2seq.jsonnet\n",
            "composedseq2seq.py  saved\t  simpleseq2seq.py\n",
            "create_bitext.py    saved_1\t  subdata\n",
            "dataset_readers     saved_2\t  test.tsv\n",
            "example.py\t    saved_3\t  train.tsv\n",
            "glove.6B.100d.txt   saved_4\t  Untitled.ipynb\n",
            "glove.6B.200d.txt   saved_5\t  validation-1.tsv\n",
            "glove.6B.300d.txt   saved_6\t  validation.tsv\n",
            "glove.6B.50d.txt    saved_7\t  wiki-auto-part-1-data.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9J6HJ53re3L",
        "outputId": "898a6db6-0865-471d-fcf6-ad0ed686aa48"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/playground/allennlp/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dividing the training files"
      ],
      "metadata": {
        "id": "78oU5J_LWZbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def num_rows(filename):\n",
        "  \"\"\"return the total number of lines in a file\"\"\"\n",
        "  file = open(filename, 'r', encoding='utf-8')\n",
        "  num_rows = 0\n",
        "  for line in file:\n",
        "    num_rows += 1\n",
        "  return num_rows"
      ],
      "metadata": {
        "id": "So2Oz91iWk2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_rows('./train.tsv'), num_rows('./validation.tsv'), num_rows('./test.tsv'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARPZV3fdW785",
        "outputId": "f143896e-a86b-44ca-b281-08fd5ad6be92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "547839 68480 68480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "547839 / 3 #60871"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "posHi8u8XSiT",
        "outputId": "44a4f418-35c7-4092-d809-e11caa2f81f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "182613.0"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir subdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRsEdhOBXkae",
        "outputId": "56213a94-4f8f-42ad-ed56-fe0be0cc2433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘subdata’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxEOETIaXqEE",
        "outputId": "89082e39-62f4-478a-aed1-6baaa9c6287b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all.tsv\t\t    saved\t      train.tsv\n",
            "composedseq2seq.py  seq2seq.jsonnet   Untitled.ipynb\n",
            "create_bitext.py    simpleseq2seq.py  validation.tsv\n",
            "dataset_readers     subdata\t      wiki-auto-part-1-data.json\n",
            "example.py\t    test.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAXLINES = 182613\n",
        "\n",
        "file = open('./train.tsv', 'r', encoding='utf-8')\n",
        "filename = 0\n",
        "for rownum, line in enumerate(file):\n",
        "  if rownum % MAXLINES == 0:\n",
        "    filename += 1\n",
        "    outfile = open('./subdata/train_large_' + str(filename) + '.tsv', 'w', encoding='utf-8')\n",
        "  outfile.write(line)\n",
        "outfile.close()\n",
        "file.close()"
      ],
      "metadata": {
        "id": "gc6LELDDX2v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6848 / 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Csfziks8zVsd",
        "outputId": "fe0194e9-ef21-4591-caa7-32724a0e98b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1712.0"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAXLINES = 1712\n",
        "\n",
        "file = open('./subdata/validation_1.tsv', 'r', encoding='utf-8')\n",
        "filename = 0\n",
        "file_num = 0\n",
        "for rownum, line in enumerate(file):\n",
        "  if rownum % MAXLINES == 0:\n",
        "    filename += 1\n",
        "    outfile = open('./subdata/validation_small' + str(filename) + '.tsv', 'w', encoding='utf-8')\n",
        "  outfile.write(line)\n",
        "outfile.close()\n",
        "file.close()"
      ],
      "metadata": {
        "id": "3iPjV51zdkbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAXLINES = 6848\n",
        "\n",
        "file = open('./test.tsv', 'r', encoding='utf-8')\n",
        "filename = 0\n",
        "for rownum, line in enumerate(file):\n",
        "  if rownum % MAXLINES == 0:\n",
        "    filename += 1\n",
        "    outfile = open('./subdata/test_' + str(filename) + '.tsv', 'w', encoding='utf-8')\n",
        "  outfile.write(line)\n",
        "outfile.close()\n",
        "file.close()"
      ],
      "metadata": {
        "id": "GQFHzmaxGqgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Required Packages\n"
      ],
      "metadata": {
        "id": "vTwjNRAWrhla"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cKiFpsv_nUij",
        "outputId": "7b85904c-9d4b-4043-a988-b5a82a79fdb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting allennlp_models\n",
            "  Downloading allennlp_models-2.10.0-py3-none-any.whl (464 kB)\n",
            "\u001b[K     |████████████████████████████████| 464 kB 27.4 MB/s \n",
            "\u001b[?25hCollecting conllu==4.4.2\n",
            "  Downloading conllu-4.4.2-py2.py3-none-any.whl (15 kB)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Collecting torch<1.12.0,>=1.7.0\n",
            "  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 750.6 MB 9.3 kB/s \n",
            "\u001b[?25hCollecting py-rouge==1.1\n",
            "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from allennlp_models) (3.7)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.5.1-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |████████████████████████████████| 431 kB 96.7 MB/s \n",
            "\u001b[?25hCollecting allennlp<2.11,>=2.10.0\n",
            "  Downloading allennlp-2.10.0-py3-none-any.whl (729 kB)\n",
            "\u001b[K     |████████████████████████████████| 729 kB 93.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (1.1.0)\n",
            "Collecting rich==12.1\n",
            "  Downloading rich-12.1.0-py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 97.2 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 84.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (4.64.1)\n",
            "Collecting tensorboardX>=1.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 84.6 MB/s \n",
            "\u001b[?25hCollecting torchvision<0.13.0,>=0.8.1\n",
            "  Downloading torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 87.9 MB/s \n",
            "\u001b[?25hCollecting transformers<4.21,>=4.1\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 81.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.16\n",
            "  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 99.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (0.3.5.1)\n",
            "Collecting requests>=2.28\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (1.7.3)\n",
            "Requirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (8.14.0)\n",
            "Collecting sentencepiece>=0.1.96\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 70.6 MB/s \n",
            "\u001b[?25hCollecting pytest>=6.2.5\n",
            "  Downloading pytest-7.1.3-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 102.3 MB/s \n",
            "\u001b[?25hCollecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.18.0.tar.gz (592 kB)\n",
            "\u001b[K     |████████████████████████████████| 592 kB 80.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (0.4.2)\n",
            "Collecting filelock<3.8,>=3.3\n",
            "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
            "Collecting protobuf==3.20.0\n",
            "  Downloading protobuf-3.20.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 89.6 MB/s \n",
            "\u001b[?25hCollecting cached-path<1.2.0,>=1.1.3\n",
            "  Downloading cached_path-1.1.6-py3-none-any.whl (26 kB)\n",
            "Collecting lmdb>=1.2.1\n",
            "  Downloading lmdb-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 104.5 MB/s \n",
            "\u001b[?25hCollecting wandb<0.13.0,>=0.10.0\n",
            "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 66.6 MB/s \n",
            "\u001b[?25hCollecting h5py>=3.6.0\n",
            "  Downloading h5py-3.7.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 62.0 MB/s \n",
            "\u001b[?25hCollecting spacy<3.4,>=2.1.0\n",
            "  Downloading spacy-3.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 47.3 MB/s \n",
            "\u001b[?25hCollecting fairscale==0.4.6\n",
            "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 99.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting base58>=2.1.1\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.11,>=2.10.0->allennlp_models) (1.21.6)\n",
            "Collecting traitlets>5.1.1\n",
            "  Downloading traitlets-5.4.0-py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 85.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich==12.1->allennlp<2.11,>=2.10.0->allennlp_models) (2.6.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<5.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from rich==12.1->allennlp<2.11,>=2.10.0->allennlp_models) (4.1.1)\n",
            "Collecting boto3<2.0,>=1.0\n",
            "  Downloading boto3-1.24.83-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 74.4 MB/s \n",
            "\u001b[?25hCollecting google-cloud-storage<3.0,>=1.32.0\n",
            "  Downloading google_cloud_storage-2.5.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 82.6 MB/s \n",
            "\u001b[?25hCollecting botocore<1.28.0,>=1.27.83\n",
            "  Downloading botocore-1.27.83-py3-none-any.whl (9.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.2 MB 57.2 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.2 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.83->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 74.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (1.35.0)\n",
            "Collecting google-resumable-media>=2.3.2\n",
            "  Downloading google_resumable_media-2.3.3-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting google-cloud-core<3.0dev,>=2.3.0\n",
            "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (1.31.6)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (2022.2.1)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (21.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (57.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (1.56.4)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (4.9)\n",
            "Collecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.16->allennlp<2.11,>=2.10.0->allennlp_models) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.16->allennlp<2.11,>=2.10.0->allennlp_models) (4.12.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.5->allennlp_models) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.5->allennlp_models) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.5->allennlp_models) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (3.0.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.0->allennlp_models) (0.4.8)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.0->allennlp_models) (2.0.1)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.0->allennlp_models) (22.1.0)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.0->allennlp_models) (1.11.0)\n",
            "Collecting iniconfig\n",
            "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.0.16->allennlp<2.11,>=2.10.0->allennlp_models) (3.8.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.28->allennlp<2.11,>=2.10.0->allennlp_models) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.28->allennlp<2.11,>=2.10.0->allennlp_models) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.28->allennlp<2.11,>=2.10.0->allennlp_models) (2.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.1->allennlp<2.11,>=2.10.0->allennlp_models) (3.1.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (0.10.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (0.7.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (0.6.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (2.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (1.0.3)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 88.8 MB/s \n",
            "\u001b[?25hCollecting thinc<8.1.0,>=8.0.14\n",
            "  Downloading thinc-8.0.17-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 93.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (3.0.10)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (2.4.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (1.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (5.2.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.13.0,>=0.8.1->allennlp<2.11,>=2.10.0->allennlp_models) (7.1.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 83.4 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 99.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.0->allennlp_models) (2.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 105.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.0->allennlp_models) (5.4.8)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets->allennlp_models) (2022.8.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets->allennlp_models) (3.8.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 95.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets->allennlp_models) (1.3.5)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 92.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->allennlp_models) (6.0.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->allennlp_models) (1.3.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->allennlp_models) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->allennlp_models) (1.8.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->allennlp_models) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->allennlp_models) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->allennlp_models) (1.2.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->allennlp_models) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.0->allennlp_models) (2.0.1)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'rich' candidate (version 12.1.0 at https://files.pythonhosted.org/packages/bc/be/1ace556afa0cf17599c2a631b04b280ae7502a9cf942c47fd66ca9ab5134/rich-12.1.0-py3-none-any.whl#sha256=b60ff99f4ff7e3d1d37444dee2b22fdd941c622dbc37841823ec1ce7f058b263 (from https://pypi.org/simple/rich/) (requires-python:>=3.6.2,<4.0.0))\n",
            "Reason for being yanked: Broken dependencies. Please upgrade to 12.2.0 or later\u001b[0m\n",
            "Building wheels for collected packages: fairscale, jsonnet, word2number, pathtools, sacremoses\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307252 sha256=cac33e466d4f427ce8febc2b3f0051a32627589ab5f52baeb5cc314d8283a863\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/4f/0b/94c29ea06dfad93260cb0377855f87b7b863312317a7f69fe7\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.18.0-cp37-cp37m-linux_x86_64.whl size=3994712 sha256=d18726390f355c1b2779cb39b3e25f480a41db6f7d8680edd96bf67b68896f87\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/63/f9/a653f9c21575e6ff271ee6a49939aa002005174cea6c35919d\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=43169a1dfff4adc12cc62f9b290e44657c04ce1d353b7801f3f52e54e5471193\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=fa2f48e150a30d32144350e59495dc4a18c9c15f1971c691fca523b7bf62fb9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=22a83ca5bbbc8d3692157f3302d0785bbc706a8b793df275b2445612855ec30b\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built fairscale jsonnet word2number pathtools sacremoses\n",
            "Installing collected packages: urllib3, protobuf, requests, jmespath, smmap, google-crc32c, botocore, s3transfer, pydantic, google-resumable-media, google-cloud-core, gitdb, filelock, commonmark, torch, tokenizers, thinc, shortuuid, setproctitle, sentry-sdk, rich, pluggy, pathtools, iniconfig, huggingface-hub, google-cloud-storage, GitPython, docker-pycreds, boto3, xxhash, wandb, transformers, traitlets, torchvision, tensorboardX, spacy, sentencepiece, sacremoses, responses, pytest, multiprocess, lmdb, jsonnet, h5py, fairscale, cached-path, base58, word2number, py-rouge, ftfy, datasets, conllu, allennlp, allennlp-models\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.9.2\n",
            "    Uninstalling pydantic-1.9.2:\n",
            "      Successfully uninstalled pydantic-1.9.2\n",
            "  Attempting uninstall: google-resumable-media\n",
            "    Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.8.0\n",
            "    Uninstalling filelock-3.8.0:\n",
            "      Successfully uninstalled filelock-3.8.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.0\n",
            "    Uninstalling thinc-8.1.0:\n",
            "      Successfully uninstalled thinc-8.1.0\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.1.1\n",
            "    Uninstalling traitlets-5.1.1:\n",
            "      Successfully uninstalled traitlets-5.1.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.1+cu113\n",
            "    Uninstalling torchvision-0.13.1+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.1+cu113\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "  Attempting uninstall: lmdb\n",
            "    Found existing installation: lmdb 0.99\n",
            "    Uninstalling lmdb-0.99:\n",
            "      Successfully uninstalled lmdb-0.99\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.11.0 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.3.3 which is incompatible.\n",
            "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.3.1 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.27 allennlp-2.10.0 allennlp-models-2.10.0 base58-2.1.1 boto3-1.24.83 botocore-1.27.83 cached-path-1.1.6 commonmark-0.9.1 conllu-4.4.2 datasets-2.5.1 docker-pycreds-0.4.0 fairscale-0.4.6 filelock-3.7.1 ftfy-6.1.1 gitdb-4.0.9 google-cloud-core-2.3.2 google-cloud-storage-2.5.0 google-crc32c-1.5.0 google-resumable-media-2.3.3 h5py-3.7.0 huggingface-hub-0.10.0 iniconfig-1.1.1 jmespath-1.0.1 jsonnet-0.18.0 lmdb-1.3.0 multiprocess-0.70.13 pathtools-0.1.2 pluggy-1.0.0 protobuf-3.20.0 py-rouge-1.1 pydantic-1.8.2 pytest-7.1.3 requests-2.28.1 responses-0.18.0 rich-12.1.0 s3transfer-0.6.0 sacremoses-0.0.53 sentencepiece-0.1.97 sentry-sdk-1.9.9 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 spacy-3.3.1 tensorboardX-2.5.1 thinc-8.0.17 tokenizers-0.12.1 torch-1.11.0 torchvision-0.12.0 traitlets-5.4.0 transformers-4.20.1 urllib3-1.26.12 wandb-0.12.21 word2number-1.1 xxhash-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "h5py",
                  "requests",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: allennlp in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: torch<1.12.0,>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.11.0)\n",
            "Requirement already satisfied: transformers<4.21,>=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.20.1)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.10.0)\n",
            "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.3.5.1)\n",
            "Requirement already satisfied: spacy<3.4,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.3.1)\n",
            "Requirement already satisfied: lmdb>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.3.0)\n",
            "Requirement already satisfied: torchvision<0.13.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.12.0)\n",
            "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.4.2)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.5.1)\n",
            "Requirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.0.53)\n",
            "Requirement already satisfied: filelock<3.8,>=3.3 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.21.6)\n",
            "Requirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.14.0)\n",
            "Requirement already satisfied: wandb<0.13.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.12.21)\n",
            "Requirement already satisfied: fairscale==0.4.6 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.4.6)\n",
            "Requirement already satisfied: h5py>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.7.0)\n",
            "Requirement already satisfied: cached-path<1.2.0,>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.1.6)\n",
            "Requirement already satisfied: rich==12.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (12.1.0)\n",
            "Requirement already satisfied: base58>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.1.1)\n",
            "Requirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (5.4.0)\n",
            "Requirement already satisfied: jsonnet>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.18.0)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.28.1)\n",
            "Requirement already satisfied: protobuf==3.20.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.20.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.96 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.1.97)\n",
            "Requirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.64.1)\n",
            "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.7/dist-packages (from allennlp) (7.1.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.7.3)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich==12.1->allennlp) (2.6.1)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from rich==12.1->allennlp) (0.9.1)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from rich==12.1->allennlp) (4.1.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.7/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (2.5.0)\n",
            "Requirement already satisfied: boto3<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (1.24.83)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.6.0)\n",
            "Requirement already satisfied: botocore<1.28.0,>=1.27.83 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.27.83)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.0.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.83->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.26.12)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.83->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.8.2)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.3.3)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.3.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.31.6)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.35.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.56.4)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (57.4.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2022.2.1)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (21.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.2.4)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.7/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.16->allennlp) (4.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.16->allennlp) (6.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.5->allennlp) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.5->allennlp) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.5->allennlp) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (3.0.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.4.8)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=6.2.5->allennlp) (2.0.1)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=6.2.5->allennlp) (1.11.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=6.2.5->allennlp) (1.0.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest>=6.2.5->allennlp) (1.1.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=6.2.5->allennlp) (22.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.0.16->allennlp) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.28->allennlp) (2022.6.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.28->allennlp) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.28->allennlp) (2.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.1->allennlp) (3.1.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.8.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.7)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (8.0.17)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.11.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.3.0)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4,>=2.1.0->allennlp) (5.2.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.13.0,>=0.8.1->allennlp) (7.1.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.21,>=4.1->allennlp) (0.12.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.3.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.1.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.9.9)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.0.9)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (3.1.27)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.4.8)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (5.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install allennlp_models\n",
        "!pip install allennlp\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIG-zgcptpr6",
        "outputId": "4ba9bc07-7bc4-429e-be69-979dc3680c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "absl-py==1.2.0\n",
            "aeppl==0.0.33\n",
            "aesara==2.7.9\n",
            "aiohttp==3.8.1\n",
            "aiosignal==1.2.0\n",
            "alabaster==0.7.12\n",
            "albumentations==1.2.1\n",
            "allennlp==2.10.0\n",
            "allennlp-models==2.10.0\n",
            "altair==4.2.0\n",
            "appdirs==1.4.4\n",
            "arviz==0.12.1\n",
            "astor==0.8.1\n",
            "astropy==4.3.1\n",
            "astunparse==1.6.3\n",
            "async-timeout==4.0.2\n",
            "asynctest==0.13.0\n",
            "atari-py==0.2.9\n",
            "atomicwrites==1.4.1\n",
            "attrs==22.1.0\n",
            "audioread==3.0.0\n",
            "autograd==1.4\n",
            "Babel==2.10.3\n",
            "backcall==0.2.0\n",
            "base58==2.1.1\n",
            "beautifulsoup4==4.6.3\n",
            "bleach==5.0.1\n",
            "blis==0.7.8\n",
            "bokeh==2.3.3\n",
            "boto3==1.24.65\n",
            "botocore==1.27.65\n",
            "branca==0.5.0\n",
            "bs4==0.0.1\n",
            "CacheControl==0.12.11\n",
            "cached-path==1.1.5\n",
            "cached-property==1.5.2\n",
            "cachetools==4.2.4\n",
            "catalogue==2.0.8\n",
            "certifi==2022.6.15\n",
            "cffi==1.15.1\n",
            "cftime==1.6.1\n",
            "chardet==3.0.4\n",
            "charset-normalizer==2.1.1\n",
            "click==7.1.2\n",
            "clikit==0.6.2\n",
            "cloudpickle==1.5.0\n",
            "cmake==3.22.6\n",
            "cmdstanpy==1.0.7\n",
            "colorcet==3.0.0\n",
            "colorlover==0.3.0\n",
            "commonmark==0.9.1\n",
            "community==1.0.0b1\n",
            "conllu==4.4.2\n",
            "cons==0.4.5\n",
            "contextlib2==0.5.5\n",
            "convertdate==2.4.0\n",
            "crashtest==0.3.1\n",
            "crcmod==1.7\n",
            "cufflinks==0.17.3\n",
            "cupy-cuda111==9.4.0\n",
            "cvxopt==1.3.0\n",
            "cvxpy==1.2.1\n",
            "cycler==0.11.0\n",
            "cymem==2.0.6\n",
            "Cython==0.29.32\n",
            "daft==0.0.4\n",
            "dask==2022.2.0\n",
            "datascience==0.17.5\n",
            "datasets==2.4.0\n",
            "debugpy==1.0.0\n",
            "decorator==4.4.2\n",
            "defusedxml==0.7.1\n",
            "descartes==1.1.0\n",
            "dill==0.3.5.1\n",
            "distributed==2022.2.0\n",
            "dlib==19.24.0\n",
            "dm-tree==0.1.7\n",
            "docker-pycreds==0.4.0\n",
            "docutils==0.17.1\n",
            "dopamine-rl==1.0.5\n",
            "earthengine-api==0.1.321\n",
            "easydict==1.9\n",
            "ecos==2.0.10\n",
            "editdistance==0.5.3\n",
            "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl\n",
            "entrypoints==0.4\n",
            "ephem==4.1.3\n",
            "et-xmlfile==1.1.0\n",
            "etils==0.7.1\n",
            "etuples==0.3.5\n",
            "fa2==0.3.5\n",
            "fairscale==0.4.6\n",
            "fastai==2.7.9\n",
            "fastcore==1.5.22\n",
            "fastdownload==0.0.7\n",
            "fastdtw==0.3.4\n",
            "fastjsonschema==2.16.1\n",
            "fastprogress==1.0.3\n",
            "fastrlock==0.8\n",
            "feather-format==0.4.1\n",
            "filelock==3.7.1\n",
            "firebase-admin==4.4.0\n",
            "fix-yahoo-finance==0.0.22\n",
            "Flask==1.1.4\n",
            "flatbuffers==2.0.7\n",
            "folium==0.12.1.post1\n",
            "frozenlist==1.3.1\n",
            "fsspec==2022.7.1\n",
            "ftfy==6.1.1\n",
            "future==0.16.0\n",
            "gast==0.5.3\n",
            "GDAL==2.2.2\n",
            "gdown==4.4.0\n",
            "gensim==3.6.0\n",
            "geographiclib==1.52\n",
            "geopy==1.17.0\n",
            "gin-config==0.5.0\n",
            "gitdb==4.0.9\n",
            "GitPython==3.1.27\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-api-core==1.31.6\n",
            "google-api-python-client==1.12.11\n",
            "google-auth==1.35.0\n",
            "google-auth-httplib2==0.0.4\n",
            "google-auth-oauthlib==0.4.6\n",
            "google-cloud-bigquery==1.21.0\n",
            "google-cloud-bigquery-storage==1.1.2\n",
            "google-cloud-core==2.3.2\n",
            "google-cloud-datastore==1.8.0\n",
            "google-cloud-firestore==1.7.0\n",
            "google-cloud-language==1.2.0\n",
            "google-cloud-storage==2.5.0\n",
            "google-cloud-translate==1.5.0\n",
            "google-colab @ file:///colabtools/dist/google-colab-1.0.0.tar.gz\n",
            "google-crc32c==1.5.0\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==2.3.3\n",
            "googleapis-common-protos==1.56.4\n",
            "googledrivedownloader==0.4\n",
            "graphviz==0.10.1\n",
            "greenlet==1.1.3\n",
            "grpcio==1.47.0\n",
            "gspread==3.4.2\n",
            "gspread-dataframe==3.0.8\n",
            "gym==0.25.2\n",
            "gym-notices==0.0.8\n",
            "h5py==3.7.0\n",
            "HeapDict==1.0.1\n",
            "hijri-converter==2.2.4\n",
            "holidays==0.15\n",
            "holoviews==1.14.9\n",
            "html5lib==1.0.1\n",
            "httpimport==0.5.18\n",
            "httplib2==0.17.4\n",
            "httplib2shim==0.0.3\n",
            "httpstan==4.6.1\n",
            "huggingface-hub==0.8.1\n",
            "humanize==0.5.1\n",
            "hyperopt==0.1.2\n",
            "idna==2.10\n",
            "imageio==2.9.0\n",
            "imagesize==1.4.1\n",
            "imbalanced-learn==0.8.1\n",
            "imblearn==0.0\n",
            "imgaug==0.4.0\n",
            "importlib-metadata==4.12.0\n",
            "importlib-resources==5.9.0\n",
            "imutils==0.5.4\n",
            "inflect==2.1.0\n",
            "iniconfig==1.1.1\n",
            "intel-openmp==2022.1.0\n",
            "intervaltree==2.1.0\n",
            "ipykernel==5.3.4\n",
            "ipython==7.9.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.3.9\n",
            "ipywidgets==7.7.1\n",
            "itsdangerous==1.1.0\n",
            "jax==0.3.14\n",
            "jaxlib @ https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.14+cuda11.cudnn805-cp37-none-manylinux2014_x86_64.whl\n",
            "jieba==0.42.1\n",
            "Jinja2==2.11.3\n",
            "jmespath==1.0.1\n",
            "joblib==1.1.0\n",
            "jpeg4py==0.1.4\n",
            "jsonnet==0.18.0\n",
            "jsonschema==4.3.3\n",
            "jupyter-client==6.1.12\n",
            "jupyter-console==6.1.0\n",
            "jupyter-core==4.11.1\n",
            "jupyterlab-widgets==3.0.2\n",
            "kaggle==1.5.12\n",
            "kapre==0.3.7\n",
            "keras==2.8.0\n",
            "Keras-Preprocessing==1.1.2\n",
            "keras-vis==0.4.1\n",
            "kiwisolver==1.4.4\n",
            "korean-lunar-calendar==0.2.1\n",
            "langcodes==3.3.0\n",
            "libclang==14.0.6\n",
            "librosa==0.8.1\n",
            "lightgbm==2.2.3\n",
            "llvmlite==0.39.0\n",
            "lmdb==1.3.0\n",
            "locket==1.0.0\n",
            "logical-unification==0.4.5\n",
            "LunarCalendar==0.0.9\n",
            "lxml==4.9.1\n",
            "Markdown==3.4.1\n",
            "MarkupSafe==2.0.1\n",
            "marshmallow==3.17.1\n",
            "matplotlib==3.2.2\n",
            "matplotlib-venn==0.11.7\n",
            "miniKanren==1.0.3\n",
            "missingno==0.5.1\n",
            "mistune==0.8.4\n",
            "mizani==0.7.3\n",
            "mkl==2019.0\n",
            "mlxtend==0.14.0\n",
            "more-itertools==8.14.0\n",
            "moviepy==0.2.3.5\n",
            "mpmath==1.2.1\n",
            "msgpack==1.0.4\n",
            "multidict==6.0.2\n",
            "multipledispatch==0.6.0\n",
            "multiprocess==0.70.13\n",
            "multitasking==0.0.11\n",
            "murmurhash==1.0.8\n",
            "music21==5.5.0\n",
            "natsort==5.5.0\n",
            "nbconvert==5.6.1\n",
            "nbformat==5.4.0\n",
            "netCDF4==1.6.0\n",
            "networkx==2.6.3\n",
            "nibabel==3.0.2\n",
            "nltk==3.7\n",
            "notebook==5.3.1\n",
            "numba==0.56.0\n",
            "numexpr==2.8.3\n",
            "numpy==1.21.6\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.2.0\n",
            "okgrade==0.4.3\n",
            "opencv-contrib-python==4.6.0.66\n",
            "opencv-python==4.6.0.66\n",
            "opencv-python-headless==4.6.0.66\n",
            "openpyxl==3.0.10\n",
            "opt-einsum==3.3.0\n",
            "osqp==0.6.2.post0\n",
            "packaging==21.3\n",
            "palettable==3.3.0\n",
            "pandas==1.3.5\n",
            "pandas-datareader==0.9.0\n",
            "pandas-gbq==0.13.3\n",
            "pandas-profiling==1.4.1\n",
            "pandocfilters==1.5.0\n",
            "panel==0.12.1\n",
            "param==1.12.2\n",
            "parso==0.8.3\n",
            "partd==1.3.0\n",
            "pastel==0.2.1\n",
            "pathlib==1.0.1\n",
            "pathtools==0.1.2\n",
            "pathy==0.6.2\n",
            "patsy==0.5.2\n",
            "pep517==0.13.0\n",
            "pexpect==4.8.0\n",
            "pickleshare==0.7.5\n",
            "Pillow==7.1.2\n",
            "pip-tools==6.2.0\n",
            "plotly==5.5.0\n",
            "plotnine==0.8.0\n",
            "pluggy==1.0.0\n",
            "pooch==1.6.0\n",
            "portpicker==1.3.9\n",
            "prefetch-generator==1.0.1\n",
            "preshed==3.0.7\n",
            "prettytable==3.4.0\n",
            "progressbar2==3.38.0\n",
            "promise==2.3\n",
            "prompt-toolkit==2.0.10\n",
            "prophet==1.1\n",
            "protobuf==3.20.0\n",
            "psutil==5.4.8\n",
            "psycopg2==2.9.3\n",
            "ptyprocess==0.7.0\n",
            "py==1.11.0\n",
            "py-rouge==1.1\n",
            "pyarrow==6.0.1\n",
            "pyasn1==0.4.8\n",
            "pyasn1-modules==0.2.8\n",
            "pycocotools==2.0.4\n",
            "pycparser==2.21\n",
            "pyct==0.4.8\n",
            "pydantic==1.8.2\n",
            "pydata-google-auth==1.4.0\n",
            "pydot==1.3.0\n",
            "pydot-ng==2.0.0\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "pyemd==0.5.1\n",
            "pyerfa==2.0.0.1\n",
            "Pygments==2.6.1\n",
            "pygobject==3.26.1\n",
            "pylev==1.4.0\n",
            "pymc==4.1.4\n",
            "PyMeeus==0.5.11\n",
            "pymongo==4.2.0\n",
            "pymystem3==0.2.0\n",
            "PyOpenGL==3.1.6\n",
            "pyparsing==3.0.9\n",
            "pyrsistent==0.18.1\n",
            "pysimdjson==3.2.0\n",
            "pysndfile==1.3.8\n",
            "PySocks==1.7.1\n",
            "pystan==3.3.0\n",
            "pytest==7.1.3\n",
            "python-apt==0.0.0\n",
            "python-chess==0.23.11\n",
            "python-dateutil==2.8.2\n",
            "python-louvain==0.16\n",
            "python-slugify==6.1.2\n",
            "python-utils==3.3.3\n",
            "pytz==2022.2.1\n",
            "pyviz-comms==2.2.1\n",
            "PyWavelets==1.3.0\n",
            "PyYAML==6.0\n",
            "pyzmq==23.2.1\n",
            "qdldl==0.1.5.post2\n",
            "qudida==0.0.4\n",
            "regex==2022.6.2\n",
            "requests==2.28.1\n",
            "requests-oauthlib==1.3.1\n",
            "resampy==0.4.0\n",
            "responses==0.18.0\n",
            "rich==12.1.0\n",
            "rpy2==3.4.5\n",
            "rsa==4.9\n",
            "s3transfer==0.6.0\n",
            "sacremoses==0.0.53\n",
            "scikit-image==0.18.3\n",
            "scikit-learn==1.0.2\n",
            "scipy==1.7.3\n",
            "screen-resolution-extra==0.0.0\n",
            "scs==3.2.0\n",
            "seaborn==0.11.2\n",
            "Send2Trash==1.8.0\n",
            "sentencepiece==0.1.97\n",
            "sentry-sdk==1.9.7\n",
            "setproctitle==1.3.2\n",
            "setuptools-git==1.2\n",
            "Shapely==1.8.4\n",
            "shortuuid==1.0.9\n",
            "six==1.15.0\n",
            "sklearn-pandas==1.8.0\n",
            "smart-open==5.2.1\n",
            "smmap==5.0.0\n",
            "snowballstemmer==2.2.0\n",
            "sortedcontainers==2.4.0\n",
            "SoundFile==0.10.3.post1\n",
            "spacy==3.3.1\n",
            "spacy-legacy==3.0.10\n",
            "spacy-loggers==1.0.3\n",
            "Sphinx==1.8.6\n",
            "sphinxcontrib-serializinghtml==1.1.5\n",
            "sphinxcontrib-websupport==1.2.4\n",
            "SQLAlchemy==1.4.40\n",
            "sqlparse==0.4.2\n",
            "srsly==2.4.4\n",
            "statsmodels==0.12.2\n",
            "sympy==1.7.1\n",
            "tables==3.7.0\n",
            "tabulate==0.8.10\n",
            "tblib==1.7.0\n",
            "tenacity==8.0.1\n",
            "tensorboard==2.8.0\n",
            "tensorboard-data-server==0.6.1\n",
            "tensorboard-plugin-wit==1.8.1\n",
            "tensorboardX==2.5.1\n",
            "tensorflow==2.8.2+zzzcolab20220719082949\n",
            "tensorflow-datasets==4.6.0\n",
            "tensorflow-estimator==2.8.0\n",
            "tensorflow-gcs-config==2.8.0\n",
            "tensorflow-hub==0.12.0\n",
            "tensorflow-io-gcs-filesystem==0.26.0\n",
            "tensorflow-metadata==1.10.0\n",
            "tensorflow-probability==0.16.0\n",
            "termcolor==1.1.0\n",
            "terminado==0.13.3\n",
            "testpath==0.6.0\n",
            "text-unidecode==1.3\n",
            "textblob==0.15.3\n",
            "thinc==8.0.17\n",
            "threadpoolctl==3.1.0\n",
            "tifffile==2021.11.2\n",
            "tokenizers==0.12.1\n",
            "toml==0.10.2\n",
            "tomli==2.0.1\n",
            "toolz==0.12.0\n",
            "torch==1.11.0\n",
            "torchaudio @ https://download.pytorch.org/whl/cu113/torchaudio-0.12.1%2Bcu113-cp37-cp37m-linux_x86_64.whl\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.13.1\n",
            "torchvision==0.12.0\n",
            "tornado==5.1.1\n",
            "tqdm==4.64.0\n",
            "traitlets==5.3.0\n",
            "transformers==4.20.1\n",
            "tweepy==3.10.0\n",
            "typeguard==2.7.1\n",
            "typer==0.4.2\n",
            "typing-extensions==4.1.1\n",
            "tzlocal==1.5.1\n",
            "ujson==5.4.0\n",
            "uritemplate==3.0.1\n",
            "urllib3==1.26.12\n",
            "vega-datasets==0.9.0\n",
            "wandb==0.12.21\n",
            "wasabi==0.10.1\n",
            "wcwidth==0.2.5\n",
            "webargs==8.2.0\n",
            "webencodings==0.5.1\n",
            "Werkzeug==1.0.1\n",
            "widgetsnbextension==3.6.1\n",
            "word2number==1.1\n",
            "wordcloud==1.8.2.2\n",
            "wrapt==1.14.1\n",
            "xarray==0.20.2\n",
            "xarray-einstats==0.2.2\n",
            "xgboost==0.90\n",
            "xkit==0.0.0\n",
            "xlrd==1.1.0\n",
            "xlwt==1.3.0\n",
            "xxhash==3.0.0\n",
            "yarl==1.8.1\n",
            "yellowbrick==1.5\n",
            "zict==2.2.0\n",
            "zipp==3.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import allennlp_models\n",
        "import allennlp"
      ],
      "metadata": {
        "id": "F44s8jWnt_cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTNyy9OlnUik"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from itertools import chain\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from typing import Iterable, Tuple\n",
        "# import dataset readers available for Seq2Seq task from models repository\n",
        "from allennlp_models.generation.dataset_readers.seq2seq import Seq2SeqDatasetReader\n",
        "from dataset_readers.seq2seq_tsv import Seq2SeqTsvReader\n",
        "\n",
        "from allennlp.data.data_loaders.data_loader import DataLoader\n",
        "# from allennlp.data.tokenizers.tokenizer import Tokenizer\n",
        "# changed tokenizer due to package update\n",
        "from allennlp.data.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
        "from allennlp.data.token_indexers.single_id_token_indexer import SingleIdTokenIndexer\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "# from allennlp.data.iterators import BucketIterator\n",
        "from allennlp.modules.token_embedders.embedding import Embedding\n",
        "from allennlp_models.rc.modules.seq2seq_encoders.stacked_self_attention import StackedSelfAttentionEncoder\n",
        "from allennlp.modules.text_field_embedders.basic_text_field_embedder import BasicTextFieldEmbedder\n",
        "from allennlp.modules.attention.dot_product_attention import DotProductAttention\n",
        "from allennlp_models.generation.models.simple_seq2seq import SimpleSeq2Seq\n",
        "from allennlp_models.generation.predictors.seq2seq import Seq2SeqPredictor\n",
        "from allennlp.training.gradient_descent_trainer import GradientDescentTrainer\n",
        "# from allennlp.training.trainer import Trainer\n",
        "# DataLoader for generating batches of Instances\n",
        "from allennlp.data.data_loaders.multiprocess_data_loader import MultiProcessDataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNJpnHQenUil",
        "outputId": "dc057619-e1c5-4390-e865-71bd10d3c19f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enabling notebook extension jupyter-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/root/.jupyter/nbconfig/notebook.json\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/root/.jupyter/nbconfig/notebook.json\n"
          ]
        }
      ],
      "source": [
        "!jupyter nbextension enable --py widgetsnbextension"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import spatial\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XkCAHX64BnRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# urllib.request.urlretrieve('https://nlp.stanford.edu/data/glove.6B.zip','glove.6B.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ms_MAle2Bv9S",
        "outputId": "a08762fa-d94a-4ab0-84ce-fdddc9553f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('glove.6B.zip', <http.client.HTTPMessage at 0x7f9335ee3ad0>)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/gdrive/MyDrive/playground/allennlp/data/glove.6B.zip\" -d \"/content/gdrive/MyDrive/playground/allennlp/data/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhp2JbhvB3cF",
        "outputId": "fb642ff3-1b21-4953-e3e4-e83404ca1cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/MyDrive/playground/allennlp/data/glove.6B.zip\n",
            "  inflating: /content/gdrive/MyDrive/playground/allennlp/data/glove.6B.50d.txt  \n",
            "  inflating: /content/gdrive/MyDrive/playground/allennlp/data/glove.6B.100d.txt  \n",
            "  inflating: /content/gdrive/MyDrive/playground/allennlp/data/glove.6B.200d.txt  \n",
            "  inflating: /content/gdrive/MyDrive/playground/allennlp/data/glove.6B.300d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emmbed_dict = {}\n",
        "with open('/content/gdrive/MyDrive/playground/allennlp/data/glove.6B.300d.txt','r') as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vector = np.asarray(values[1:],'float32')\n",
        "    emmbed_dict[word]=vector"
      ],
      "metadata": {
        "id": "XPEqkPjuCVzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MR01scysnUil",
        "outputId": "dfa6f9d8-91aa-44d7-fb6e-5a713c7fd19c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.get_device_name()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model for Complexity"
      ],
      "metadata": {
        "id": "etm2xnvx3HoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okFwXZzkCorF",
        "outputId": "9ee3b35b-2c75-4603-9c5b-e8013b147e1e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/playground/allennlp/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/playground/allennlp/cwi-master/CWI Sequence Labeller/sequence-labeler-master')\n",
        "\n",
        "from complex_labeller import Complexity_labeller\n",
        "model_path = '/content/gdrive/MyDrive/playground/allennlp/cwi-master/CWI Sequence Labeller/cwi_seq.model'\n",
        "temp_path = '/content/gdrive/MyDrive/playground/allennlp/cwi-master/CWI Sequence Labeller/temp_file.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGINaYlq3Njz",
        "outputId": "ea6e2ee8-9b0a-4a36-8c07-3e7e345932f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_comp = Complexity_labeller(model_path, temp_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIxPQblW3Zq7",
        "outputId": "ee607ab4-9c65-4ba0-efa4-ee5d4ca404a5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/gdrive/MyDrive/playground/allennlp/cwi-master/CWI Sequence Labeller/sequence-labeler-master/labeler.py:141: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  reuse=False)\n",
            "/content/gdrive/MyDrive/playground/allennlp/cwi-master/CWI Sequence Labeller/sequence-labeler-master/labeler.py:146: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  reuse=False)\n",
            "WARNING:tensorflow:From /content/gdrive/MyDrive/playground/allennlp/cwi-master/CWI Sequence Labeller/sequence-labeler-master/labeler.py:148: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/rnn.py:446: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:988: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  partitioner=maybe_partitioner)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:992: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:996: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  initializer=initializer)\n",
            "/content/gdrive/MyDrive/playground/allennlp/cwi-master/CWI Sequence Labeller/sequence-labeler-master/labeler.py:161: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  char_output_tensor = tf.layers.dense(char_output_tensor, char_hidden_layer_size, activation=tf.tanh, kernel_initializer=self.initializer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/content/gdrive/MyDrive/playground/allennlp/cwi-master/CWI Sequence Labeller/sequence-labeler-master/labeler.py:178: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  attention_output = tf.layers.dense(attention_evidence_tensor, self.config[\"word_embedding_size\"], activation=tf.tanh, kernel_initializer=self.initializer)\n",
            "/content/gdrive/MyDrive/playground/allennlp/cwi-master/CWI Sequence Labeller/sequence-labeler-master/labeler.py:179: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  attention_output = tf.layers.dense(attention_output, self.config[\"word_embedding_size\"], activation=tf.sigmoid, kernel_initializer=self.initializer)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "/content/gdrive/MyDrive/playground/allennlp/cwi-master/CWI Sequence Labeller/sequence-labeler-master/labeler.py:193: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  reuse=False)\n",
            "/content/gdrive/MyDrive/playground/allennlp/cwi-master/CWI Sequence Labeller/sequence-labeler-master/labeler.py:198: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  reuse=False)\n",
            "/content/gdrive/MyDrive/playground/allennlp/cwi-master/CWI Sequence Labeller/sequence-labeler-master/labeler.py:259: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  lmcost_hidden_layer = tf.layers.dense(input_tensor, self.config[\"lmcost_hidden_layer_size\"], activation=tf.tanh, kernel_initializer=self.initializer)\n",
            "/content/gdrive/MyDrive/playground/allennlp/cwi-master/CWI Sequence Labeller/sequence-labeler-master/labeler.py:260: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  lmcost_output = tf.layers.dense(lmcost_hidden_layer, lmcost_max_vocab_size, activation=None, kernel_initializer=self.initializer)\n",
            "/content/gdrive/MyDrive/playground/allennlp/cwi-master/CWI Sequence Labeller/sequence-labeler-master/labeler.py:216: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  processed_tensor = tf.layers.dense(processed_tensor, self.config[\"hidden_layer_size\"], activation=tf.tanh, kernel_initializer=self.initializer)\n",
            "/content/gdrive/MyDrive/playground/allennlp/cwi-master/CWI Sequence Labeller/sequence-labeler-master/labeler.py:219: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.scores = tf.layers.dense(processed_tensor, len(self.label2id), activation=None, kernel_initializer=self.initializer, name=\"output_ff\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two options when converting text to CoNLL-type tab-separated format:\n",
        "\n",
        "- `convert_format_string`\n",
        "- `convert_format_token`"
      ],
      "metadata": {
        "id": "ACJTx0Up3yLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Complexity_labeller.convert_format_string(model_comp, \"The orchestra's first broadcast was on November 13, 1937, and it continued until disbanded in 1954.\")"
      ],
      "metadata": {
        "id": "GMyFqXPr3te7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Complexity_labeller.convert_format_token(model_comp, ['You','can','convert','tokens','like','this'])"
      ],
      "metadata": {
        "id": "oaA5KEyP4FW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the text has been converted there are three methods to access complexity information:\n",
        "\n",
        "- `get_dataframe`\n",
        "- `get_bin_labels`\n",
        "- `get_prob_labels`"
      ],
      "metadata": {
        "id": "dWD-FE4C4Gil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting example sentence:'Based in an armoured train parked in its sidings, he met with numerous ministers'\n",
        "\n",
        "Complexity_labeller.convert_format_string(model_comp,'Based in an armoured train parked in its sidings, he met with numerous ministers')"
      ],
      "metadata": {
        "id": "kJK1HpvL4Rat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = Complexity_labeller.get_dataframe(model_comp)"
      ],
      "metadata": {
        "id": "Z_b8cPww4Y0c"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "5T2Hyalc4c8p",
        "outputId": "86ef4232-04d6-4cab-8687-cf5a83cd9109"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index                                          sentences  \\\n",
              "0      0  [The, orchestra, 's, first, broadcast, was, on...   \n",
              "\n",
              "                                              labels  \\\n",
              "0  [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
              "\n",
              "                                               probs  \n",
              "0  [[0.99981886, 0.00018116605], [0.111193225, 0....  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dfc3ddf0-5638-4b95-b21d-cb0e8bafd7b9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>sentences</th>\n",
              "      <th>labels</th>\n",
              "      <th>probs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[The, orchestra, 's, first, broadcast, was, on...</td>\n",
              "      <td>[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "      <td>[[0.99981886, 0.00018116605], [0.111193225, 0....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dfc3ddf0-5638-4b95-b21d-cb0e8bafd7b9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dfc3ddf0-5638-4b95-b21d-cb0e8bafd7b9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dfc3ddf0-5638-4b95-b21d-cb0e8bafd7b9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(zip(dataframe['sentences'].values[0],dataframe['labels'].values[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzpxNqV24iGb",
        "outputId": "81db54e1-53a8-4881-df5d-74a0417db616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('You', 0),\n",
              " ('can', 0),\n",
              " ('convert', 1),\n",
              " ('a', 0),\n",
              " ('string', 1),\n",
              " ('like', 0),\n",
              " ('this', 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Complexity_labeller.get_bin_labels(model_comp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4_mACWN4suM",
        "outputId": "71f4e034-2a0e-4adb-8a22-4e89d2022c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0])]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Complexity_labeller.get_prob_labels(model_comp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj4-BrLa42Nb",
        "outputId": "d62988cd-23bf-44f7-9150-6de9f71ce5fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.00012409111,\n",
              " 6.983685e-05,\n",
              " 0.7266071,\n",
              " 0.0002355496,\n",
              " 0.7401603,\n",
              " 0.00036351883,\n",
              " 0.00010540021]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model for Seq2Seq Simplification"
      ],
      "metadata": {
        "id": "0xwkS_qt-DG7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdZctjhYnUim"
      },
      "outputs": [],
      "source": [
        "def build_data_loaders(\n",
        "    reader,\n",
        "    train_data_path: str,\n",
        "    validation_data_path: str,\n",
        ") -> Tuple[DataLoader, DataLoader]:\n",
        "    train_loader = MultiProcessDataLoader(\n",
        "        # batch size changing from 2 to 8\n",
        "        reader, train_data_path, batch_size=8, shuffle=True\n",
        "    )\n",
        "    train_loader.set_target_device(torch.device(\"cuda\"))\n",
        "    dev_loader = MultiProcessDataLoader(\n",
        "        reader, validation_data_path, batch_size=8, shuffle=False\n",
        "    )\n",
        "    dev_loader.set_target_device(torch.device(\"cuda\"))\n",
        "    return train_loader, dev_loader\n",
        "\n",
        "def build_vocab(train_loader, dev_loader) -> Vocabulary:\n",
        "    print(\"Building the vocabulary\")\n",
        "    return Vocabulary.from_instances(\n",
        "        chain(train_loader.iter_instances(), dev_loader.iter_instances())\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qibIrB08nUin"
      },
      "outputs": [],
      "source": [
        "COMP_EMBEDDING_DIM = 300\n",
        "SIMP_EMBEDDING_DIM = 300\n",
        "HIDDEN_DIM = 256\n",
        "CUDA_DEVICE = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJOYhN8AzRVf",
        "outputId": "ca356597-f1f6-4d76-d0a1-2981bbfd310d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all.tsv\t\t    glove.6B.50d.txt  subdata\n",
            "composedseq2seq.py  glove.6B.zip      test.tsv\n",
            "create_bitext.py    saved\t      train.tsv\n",
            "dataset_readers     saved_1\t      Untitled.ipynb\n",
            "example.py\t    saved_2\t      validation-1.tsv\n",
            "glove.6B.100d.txt   saved_3\t      validation.tsv\n",
            "glove.6B.200d.txt   seq2seq.jsonnet   wiki-auto-part-1-data.json\n",
            "glove.6B.300d.txt   simpleseq2seq.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-faFSCrmnUin",
        "outputId": "c3ce9573-c329-4129-e9a6-04494475fbf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 60836it [00:08, 7549.29it/s]\n",
            "loading instances: 1712it [00:00, 4024.09it/s]\n",
            "building vocab: 3456it [00:00, 34554.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building the vocabulary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "building vocab: 62548it [00:01, 31799.75it/s]\n"
          ]
        }
      ],
      "source": [
        "# reader = Seq2SeqTsvReader(\n",
        "#     source_tokenizer=WhitespaceTokenizer(),\n",
        "#     target_tokenizer=WhitespaceTokenizer(),\n",
        "#     source_token_indexers={'source_tokens': SingleIdTokenIndexer(namespace='source_tokens')},\n",
        "#     target_token_indexers={'target_tokens': SingleIdTokenIndexer(namespace='target_tokens')}\n",
        "# )\n",
        "reader = Seq2SeqTsvReader(\n",
        "    source_tokenizer=WhitespaceTokenizer(),\n",
        "    target_tokenizer=WhitespaceTokenizer(),\n",
        "    source_token_indexers={'tokens': SingleIdTokenIndexer(namespace='tokens')},\n",
        "    target_token_indexers={'tokens': SingleIdTokenIndexer(namespace='tokens')}\n",
        ")\n",
        "# The returned dataset is an iterable of Instances\n",
        "# train_dataset = reader.read('./simplification.train.tsv')   # type: Generator\n",
        "# validation_dataset = reader.read('./simplification.dev.tsv')\n",
        "\n",
        "# pass a collection of instances to v.from_instances method to create a Vocabulary object\n",
        "# map strings to integers automatically, using different namespaces\n",
        "\n",
        "train_loader, dev_loader = build_data_loaders(\n",
        "    reader, './subdata/train_1.tsv', './subdata/validation_small1.tsv'\n",
        ")\n",
        "vocab = build_vocab(train_loader, dev_loader)\n",
        "# TODO: move to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.get_namespaces()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCHKmcMZYYNQ",
        "outputId": "1d93ae52-3ab1-4aa3-8df9-ce73f8136d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tokens'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.get_index_to_token_vocabulary('tokens')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBCblXKRYk0Z",
        "outputId": "15d38cea-21b3-4344-945d-520a5f3d128a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '@@PADDING@@',\n",
              " 1: '@@UNKNOWN@@',\n",
              " 2: 'the',\n",
              " 3: 'of',\n",
              " 4: 'and',\n",
              " 5: 'in',\n",
              " 6: '@start@',\n",
              " 7: '@end@',\n",
              " 8: 'a',\n",
              " 9: 'is',\n",
              " 10: 'to',\n",
              " 11: 'was',\n",
              " 12: 'The',\n",
              " 13: 'as',\n",
              " 14: 'on',\n",
              " 15: 'for',\n",
              " 16: 'by',\n",
              " 17: 'from',\n",
              " 18: 'with',\n",
              " 19: 'an',\n",
              " 20: 'at',\n",
              " 21: 'that',\n",
              " 22: 'his',\n",
              " 23: 'He',\n",
              " 24: 'are',\n",
              " 25: 'or',\n",
              " 26: 'In',\n",
              " 27: 'It',\n",
              " 28: 'he',\n",
              " 29: 'which',\n",
              " 30: 'also',\n",
              " 31: 'were',\n",
              " 32: 'it',\n",
              " 33: 'has',\n",
              " 34: 'first',\n",
              " 35: 'American',\n",
              " 36: 'be',\n",
              " 37: 'who',\n",
              " 38: '–',\n",
              " 39: 'known',\n",
              " 40: 'had',\n",
              " 41: 'one',\n",
              " 42: 'United',\n",
              " 43: 'its',\n",
              " 44: 'have',\n",
              " 45: 'born',\n",
              " 46: 'her',\n",
              " 47: 'their',\n",
              " 48: 'but',\n",
              " 49: 'after',\n",
              " 50: 'two',\n",
              " 51: '(born',\n",
              " 52: 'not',\n",
              " 53: 'other',\n",
              " 54: 'been',\n",
              " 55: 'most',\n",
              " 56: 'She',\n",
              " 57: 'New',\n",
              " 58: 'A',\n",
              " 59: 'died',\n",
              " 60: 'into',\n",
              " 61: 'used',\n",
              " 62: 'they',\n",
              " 63: 'became',\n",
              " 64: 'city',\n",
              " 65: 'called',\n",
              " 66: 'January',\n",
              " 67: 'when',\n",
              " 68: 'about',\n",
              " 69: 'May',\n",
              " 70: 'more',\n",
              " 71: 'made',\n",
              " 72: 'On',\n",
              " 73: '\"The',\n",
              " 74: 'March',\n",
              " 75: 'April',\n",
              " 76: 'between',\n",
              " 77: ',',\n",
              " 78: 'she',\n",
              " 79: 'part',\n",
              " 80: 'July',\n",
              " 81: 'can',\n",
              " 82: 'than',\n",
              " 83: 'during',\n",
              " 84: 'many',\n",
              " 85: 'this',\n",
              " 86: 'June',\n",
              " 87: 'This',\n",
              " 88: 'August',\n",
              " 89: 'until',\n",
              " 90: 'September',\n",
              " 91: 'all',\n",
              " 92: 'November',\n",
              " 93: 'over',\n",
              " 94: 'They',\n",
              " 95: 'people',\n",
              " 96: 'played',\n",
              " 97: 'December',\n",
              " 98: 'October',\n",
              " 99: 'only',\n",
              " 100: 'such',\n",
              " 101: 'three',\n",
              " 102: 'February',\n",
              " 103: 'where',\n",
              " 104: 'released',\n",
              " 105: 'States',\n",
              " 106: 'former',\n",
              " 107: 'name',\n",
              " 108: 'won',\n",
              " 109: 'years',\n",
              " 110: 'some',\n",
              " 111: 'British',\n",
              " 112: 'National',\n",
              " 113: ';',\n",
              " 114: 'age',\n",
              " 115: 'World',\n",
              " 116: '-',\n",
              " 117: 'film',\n",
              " 118: 'state',\n",
              " 119: 'served',\n",
              " 120: 'up',\n",
              " 121: 'second',\n",
              " 122: 'including',\n",
              " 123: 'de',\n",
              " 124: 'through',\n",
              " 125: 'found',\n",
              " 126: 'then',\n",
              " 127: 'South',\n",
              " 128: 'later',\n",
              " 129: 'University',\n",
              " 130: 'time',\n",
              " 131: 'being',\n",
              " 132: 'U.S.',\n",
              " 133: 'named',\n",
              " 134: 'since',\n",
              " 135: 'series',\n",
              " 136: 'would',\n",
              " 137: 'located',\n",
              " 138: 'number',\n",
              " 139: 'new',\n",
              " 140: 'English',\n",
              " 141: 'before',\n",
              " 142: 'under',\n",
              " 143: 'both',\n",
              " 144: '(',\n",
              " 145: 'county',\n",
              " 146: 'France.',\n",
              " 147: 'population',\n",
              " 148: 'department',\n",
              " 149: 'member',\n",
              " 150: 'After',\n",
              " 151: 'area',\n",
              " 152: 'football',\n",
              " 153: 'four',\n",
              " 154: 'television',\n",
              " 155: 'His',\n",
              " 156: 'several',\n",
              " 157: 'John',\n",
              " 158: 'best',\n",
              " 159: 'President',\n",
              " 160: 'French',\n",
              " 161: 'North',\n",
              " 162: 'may',\n",
              " 163: 'As',\n",
              " 164: 'there',\n",
              " 165: 'out',\n",
              " 166: 'well',\n",
              " 167: 'began',\n",
              " 168: 'while',\n",
              " 169: 'him',\n",
              " 170: 'family',\n",
              " 171: 'early',\n",
              " 172: 'region',\n",
              " 173: 'album',\n",
              " 174: 'German',\n",
              " 175: 'around',\n",
              " 176: 'town',\n",
              " 177: 'professional',\n",
              " 178: 'largest',\n",
              " 179: 'often',\n",
              " 180: 'species',\n",
              " 181: 'same',\n",
              " 182: '1',\n",
              " 183: 'because',\n",
              " 184: 'work',\n",
              " 185: 'national',\n",
              " 186: 'movie',\n",
              " 187: 'York',\n",
              " 188: 'small',\n",
              " 189: 'group',\n",
              " 190: 'no',\n",
              " 191: ')',\n",
              " 192: 'based',\n",
              " 193: 'against',\n",
              " 194: 'large',\n",
              " 195: 'There',\n",
              " 196: 'team',\n",
              " 197: 'married',\n",
              " 198: '2010',\n",
              " 199: 'States.',\n",
              " 200: 'use',\n",
              " 201: 'main',\n",
              " 202: 'district',\n",
              " 203: 'At',\n",
              " 204: 'million',\n",
              " 205: 'band',\n",
              " 206: 'County',\n",
              " 207: 'song',\n",
              " 208: 'them',\n",
              " 209: 'Minister',\n",
              " 210: 'commune',\n",
              " 211: 'started',\n",
              " 212: 'home',\n",
              " 213: 'now',\n",
              " 214: 'municipality',\n",
              " 215: 'near',\n",
              " 216: 'these',\n",
              " 217: 'along',\n",
              " 218: 'very',\n",
              " 219: 'State',\n",
              " 220: 'Japanese',\n",
              " 221: 'any',\n",
              " 222: 'created',\n",
              " 223: 'usually',\n",
              " 224: 'held',\n",
              " 225: 'game',\n",
              " 226: 'War',\n",
              " 227: 'year',\n",
              " 228: 'seat',\n",
              " 229: 'music',\n",
              " 230: 'major',\n",
              " 231: 'so',\n",
              " 232: 'capital',\n",
              " 233: 'written',\n",
              " 234: 'each',\n",
              " 235: 'form',\n",
              " 236: 'League',\n",
              " 237: 'will',\n",
              " 238: 'published',\n",
              " 239: 'moved',\n",
              " 240: 'County,',\n",
              " 241: 'third',\n",
              " 242: 'founded',\n",
              " 243: 'place',\n",
              " 244: 'different',\n",
              " 245: 'government',\n",
              " 246: 'Its',\n",
              " 247: 'received',\n",
              " 248: 'include',\n",
              " 249: 'produced',\n",
              " 250: 'took',\n",
              " 251: 'did',\n",
              " 252: 'worked',\n",
              " 253: 'role',\n",
              " 254: 'following',\n",
              " 255: 'make',\n",
              " 256: 'like',\n",
              " 257: 'formed',\n",
              " 258: 'built',\n",
              " 259: 'five',\n",
              " 260: 'When',\n",
              " 261: 'death',\n",
              " 262: 'During',\n",
              " 263: 'City',\n",
              " 264: 'single',\n",
              " 265: 'left',\n",
              " 266: 'politician',\n",
              " 267: 'could',\n",
              " 268: 'James',\n",
              " 269: '10',\n",
              " 270: 'last',\n",
              " 271: '3',\n",
              " 272: 'popular',\n",
              " 273: 'son',\n",
              " 274: 'end',\n",
              " 275: 'elected',\n",
              " 276: 'set',\n",
              " 277: '(]',\n",
              " 278: 'political',\n",
              " 279: 'player',\n",
              " 280: '4',\n",
              " 281: 'European',\n",
              " 282: '2',\n",
              " 283: 'long',\n",
              " 284: 'given',\n",
              " 285: 'went',\n",
              " 286: 'led',\n",
              " 287: 'within',\n",
              " 288: 'River',\n",
              " 289: 'due',\n",
              " 290: 'common',\n",
              " 291: 'I',\n",
              " 292: 'north',\n",
              " 293: 'show',\n",
              " 294: 'northern',\n",
              " 295: 'much',\n",
              " 296: 'important',\n",
              " 297: 'International',\n",
              " 298: 'William',\n",
              " 299: 'public',\n",
              " 300: 'These',\n",
              " 301: 'late',\n",
              " 302: 'Canadian',\n",
              " 303: 'aged',\n",
              " 304: 'system',\n",
              " 305: 'own',\n",
              " 306: 'However,',\n",
              " 307: 'said',\n",
              " 308: 'season',\n",
              " 309: 'six',\n",
              " 310: 'making',\n",
              " 311: 'become',\n",
              " 312: 'career',\n",
              " 313: 'members',\n",
              " 314: 'House',\n",
              " 315: 'West',\n",
              " 316: 'term',\n",
              " 317: 'high',\n",
              " 318: 'Australian',\n",
              " 319: 'King',\n",
              " 320: 'water',\n",
              " 321: 'km',\n",
              " 322: 'came',\n",
              " 323: 'Award',\n",
              " 324: 'directed',\n",
              " 325: 'land',\n",
              " 326: 'appeared',\n",
              " 327: 'rock',\n",
              " 328: 'Dutch',\n",
              " 329: 'province',\n",
              " 330: 'back',\n",
              " 331: 'sometimes',\n",
              " 332: '20',\n",
              " 333: '5',\n",
              " 334: 'using',\n",
              " 335: 'Party',\n",
              " 336: 'still',\n",
              " 337: 'player.',\n",
              " 338: 'George',\n",
              " 339: 'original',\n",
              " 340: 'central',\n",
              " 341: '11',\n",
              " 342: '.',\n",
              " 343: '15',\n",
              " 344: 'world',\n",
              " 345: 'Some',\n",
              " 346: '16',\n",
              " 347: 'modern',\n",
              " 348: 'developed',\n",
              " 349: 'southern',\n",
              " 350: 'lived',\n",
              " 351: 'included',\n",
              " 352: 'book',\n",
              " 353: '1,',\n",
              " 354: 'wrote',\n",
              " 355: 'East',\n",
              " 356: 'Prime',\n",
              " 357: 'established',\n",
              " 358: 'village',\n",
              " 359: '12',\n",
              " 360: 'total',\n",
              " 361: 'Robert',\n",
              " 362: 'announced',\n",
              " 363: 'live',\n",
              " 364: 'school',\n",
              " 365: 'considered',\n",
              " 366: 'language',\n",
              " 367: '6',\n",
              " 368: 'word',\n",
              " 369: 'another',\n",
              " 370: 'international',\n",
              " 371: 'studio',\n",
              " 372: 'Most',\n",
              " 373: 'record',\n",
              " 374: 'London',\n",
              " 375: 'having',\n",
              " 376: 'those',\n",
              " 377: '30',\n",
              " 378: 'Roman',\n",
              " 379: 'few',\n",
              " 380: 'video',\n",
              " 381: 'just',\n",
              " 382: 'line',\n",
              " 383: 'if',\n",
              " 384: 'For',\n",
              " 385: 'famous',\n",
              " 386: 'Italian',\n",
              " 387: 'south',\n",
              " 388: 'military',\n",
              " 389: 'west',\n",
              " 390: '7',\n",
              " 391: 'various',\n",
              " 392: '2017)',\n",
              " 393: 'France',\n",
              " 394: 'Los',\n",
              " 395: 'title',\n",
              " 396: 'From',\n",
              " 397: 'One',\n",
              " 398: '2018)',\n",
              " 399: '2,',\n",
              " 400: '18',\n",
              " 401: 'San',\n",
              " 402: 'Great',\n",
              " 403: 'singer',\n",
              " 404: 'years.',\n",
              " 405: 'Indian',\n",
              " 406: '14',\n",
              " 407: 'retired',\n",
              " 408: 'head',\n",
              " 409: 'club',\n",
              " 410: 'what',\n",
              " 411: 'top',\n",
              " 412: '100',\n",
              " 413: 'among',\n",
              " 414: ']',\n",
              " 415: 'short',\n",
              " 416: 'St.',\n",
              " 417: 'Academy',\n",
              " 418: 'Best',\n",
              " 419: 'David',\n",
              " 420: 'life',\n",
              " 421: 'do',\n",
              " 422: 'sold',\n",
              " 423: '2011',\n",
              " 424: 'order',\n",
              " 425: 'originally',\n",
              " 426: 'play',\n",
              " 427: 'Republic',\n",
              " 428: '4,',\n",
              " 429: 'island',\n",
              " 430: 'office',\n",
              " 431: 'England',\n",
              " 432: 'every',\n",
              " 433: 'next',\n",
              " 434: 'England.',\n",
              " 435: 'opened',\n",
              " 436: '6,',\n",
              " 437: 'president',\n",
              " 438: 'playing',\n",
              " 439: 'Charles',\n",
              " 440: '&',\n",
              " 441: 'joined',\n",
              " 442: 'currently',\n",
              " 443: '9',\n",
              " 444: 'type',\n",
              " 445: '8,',\n",
              " 446: '14,',\n",
              " 447: 'commonly',\n",
              " 448: 'Her',\n",
              " 449: 'ice',\n",
              " 450: 'father',\n",
              " 451: 'body',\n",
              " 452: 'off',\n",
              " 453: 'person',\n",
              " 454: 'living',\n",
              " 455: 'current',\n",
              " 456: 'even',\n",
              " 457: '9,',\n",
              " 458: 'east',\n",
              " 459: '13,',\n",
              " 460: 'again',\n",
              " 461: '2018',\n",
              " 462: 'Many',\n",
              " 463: 'Russian',\n",
              " 464: 'similar',\n",
              " 465: 'Western',\n",
              " 466: 'fourth',\n",
              " 467: 'School',\n",
              " 468: '25',\n",
              " 469: '8',\n",
              " 470: 'Spanish',\n",
              " 471: '15,',\n",
              " 472: '17,',\n",
              " 473: 'period',\n",
              " 474: 'company',\n",
              " 475: 'parts',\n",
              " 476: '(also',\n",
              " 477: 'country',\n",
              " 478: '26,',\n",
              " 479: 'times',\n",
              " 480: 'politician.',\n",
              " 481: '2014',\n",
              " 482: 'local',\n",
              " 483: '13',\n",
              " 484: 'although',\n",
              " 485: 'debut',\n",
              " 486: '20,',\n",
              " 487: '2006',\n",
              " 488: '21',\n",
              " 489: '10,',\n",
              " 490: 'Democratic',\n",
              " 491: 'An',\n",
              " 492: 'station',\n",
              " 493: '2010,',\n",
              " 494: 'First',\n",
              " 495: 'recorded',\n",
              " 496: 'Royal',\n",
              " 497: '24',\n",
              " 498: '3,',\n",
              " 499: '2017',\n",
              " 500: '(the',\n",
              " 501: 'includes',\n",
              " 502: 'down',\n",
              " 503: 'Paul',\n",
              " 504: 'performed',\n",
              " 505: 'commune.',\n",
              " 506: 'century',\n",
              " 507: 'without',\n",
              " 508: '17',\n",
              " 509: 'General',\n",
              " 510: 'natural',\n",
              " 511: 'Michael',\n",
              " 512: 'Greek',\n",
              " 513: 'eastern',\n",
              " 514: '28',\n",
              " 515: 'less',\n",
              " 516: '(or',\n",
              " 517: 'caused',\n",
              " 518: '11,',\n",
              " 519: 'lead',\n",
              " 520: '12,',\n",
              " 521: 'though',\n",
              " 522: 'children',\n",
              " 523: 'mostly',\n",
              " 524: 'seven',\n",
              " 525: 'appointed',\n",
              " 526: 'US',\n",
              " 527: 'day',\n",
              " 528: 'human',\n",
              " 529: 'history',\n",
              " 530: '22',\n",
              " 531: 'Louis',\n",
              " 532: 'Army',\n",
              " 533: '2019)',\n",
              " 534: 'river',\n",
              " 535: 'old',\n",
              " 536: 'games',\n",
              " 537: 'America',\n",
              " 538: 'referred',\n",
              " 539: '7,',\n",
              " 540: 'civil',\n",
              " 541: 'general',\n",
              " 542: 'take',\n",
              " 543: 'Central',\n",
              " 544: 'actress',\n",
              " 545: 'character',\n",
              " 546: 'either',\n",
              " 547: 'According',\n",
              " 548: 'Christian',\n",
              " 549: 'By',\n",
              " 550: 'heart',\n",
              " 551: 'version',\n",
              " 552: '23',\n",
              " 553: 'tropical',\n",
              " 554: 'event',\n",
              " 555: 'stage',\n",
              " 556: 'Kingdom',\n",
              " 557: 'means',\n",
              " 558: 'gave',\n",
              " 559: '29,',\n",
              " 560: 'plays',\n",
              " 561: 'western',\n",
              " 562: 'II',\n",
              " 563: '18,',\n",
              " 564: 'works',\n",
              " 565: 'hockey',\n",
              " 566: 'Thomas',\n",
              " 567: 'daughter',\n",
              " 568: 'final',\n",
              " 569: '27',\n",
              " 570: 'Lake',\n",
              " 571: 'Governor',\n",
              " 572: 'across',\n",
              " 573: 'actor',\n",
              " 574: 'Union',\n",
              " 575: 'center',\n",
              " 576: 'awarded',\n",
              " 577: 'States,',\n",
              " 578: '2010.',\n",
              " 579: 'time,',\n",
              " 580: 'way',\n",
              " 581: '21,',\n",
              " 582: 'District',\n",
              " 583: 'signed',\n",
              " 584: '26',\n",
              " 585: 'time.',\n",
              " 586: 'power',\n",
              " 587: 'working',\n",
              " 588: 'generally',\n",
              " 589: 'meaning',\n",
              " 590: 'reached',\n",
              " 591: 'days',\n",
              " 592: 'wife',\n",
              " 593: 'official',\n",
              " 594: 'Cup',\n",
              " 595: 'All',\n",
              " 596: 'killed',\n",
              " 597: '2017,',\n",
              " 598: 'Summer',\n",
              " 599: '22,',\n",
              " 600: 'production',\n",
              " 601: '28,',\n",
              " 602: '25,',\n",
              " 603: 'canton',\n",
              " 604: 'side',\n",
              " 605: '2007',\n",
              " 606: 'Secretary',\n",
              " 607: 'story',\n",
              " 608: 'Richard',\n",
              " 609: 'almost',\n",
              " 610: 'attack',\n",
              " 611: 'Soviet',\n",
              " 612: 'white',\n",
              " 613: '2008',\n",
              " 614: 'Olympic',\n",
              " 615: 'chemical',\n",
              " 616: 'studied',\n",
              " 617: 'comedy',\n",
              " 618: '2005',\n",
              " 619: 'Chicago',\n",
              " 620: 'position',\n",
              " 621: '2012',\n",
              " 622: 'cancer',\n",
              " 623: 'California',\n",
              " 624: 'Football',\n",
              " 625: '24,',\n",
              " 626: 'years,',\n",
              " 627: 'how',\n",
              " 628: 'per',\n",
              " 629: 'Australia',\n",
              " 630: 'Chinese',\n",
              " 631: 'continued',\n",
              " 632: 'lost',\n",
              " 633: 'returned',\n",
              " 634: 'building',\n",
              " 635: '27,',\n",
              " 636: 'match',\n",
              " 637: 'Hall',\n",
              " 638: 'Although',\n",
              " 639: '2013',\n",
              " 640: 'young',\n",
              " 641: 'designed',\n",
              " 642: 'highest',\n",
              " 643: 'Air',\n",
              " 644: '19',\n",
              " 645: 'Church',\n",
              " 646: 'album,',\n",
              " 647: '2000',\n",
              " 648: 'genus',\n",
              " 649: 'leader',\n",
              " 650: 'Hockey',\n",
              " 651: 'party',\n",
              " 652: 'Park',\n",
              " 653: 'Northern',\n",
              " 654: 'census,',\n",
              " 655: 'Henry',\n",
              " 656: 'list',\n",
              " 657: 'Sir',\n",
              " 658: '23,',\n",
              " 659: 'Republican',\n",
              " 660: 'seen',\n",
              " 661: '2009',\n",
              " 662: 'discovered',\n",
              " 663: 'Swedish',\n",
              " 664: 'never',\n",
              " 665: 'director',\n",
              " 666: 'successful',\n",
              " 667: 'la',\n",
              " 668: '30,',\n",
              " 669: 'female',\n",
              " 670: 'Germany',\n",
              " 671: 'musical',\n",
              " 672: 'mother',\n",
              " 673: 'great',\n",
              " 674: 'actor,',\n",
              " 675: 'get',\n",
              " 676: 'Edward',\n",
              " 677: 'university',\n",
              " 678: 'With',\n",
              " 679: 'serving',\n",
              " 680: '2016,',\n",
              " 681: '16,',\n",
              " 682: 'thought',\n",
              " 683: 'support',\n",
              " 684: 'books',\n",
              " 685: 'College',\n",
              " 686: 'eight',\n",
              " 687: 'does',\n",
              " 688: 'world.',\n",
              " 689: '2011,',\n",
              " 690: '5,',\n",
              " 691: 'control',\n",
              " 692: 'taken',\n",
              " 693: '2015',\n",
              " 694: 'development',\n",
              " 695: 'outside',\n",
              " 696: 'White',\n",
              " 697: 'voice',\n",
              " 698: 'close',\n",
              " 699: 'range',\n",
              " 700: 'race',\n",
              " 701: '2016',\n",
              " 702: 'Since',\n",
              " 703: 'actress.',\n",
              " 704: 'better',\n",
              " 705: 'help',\n",
              " 706: 'states',\n",
              " 707: 'team.',\n",
              " 708: 'described',\n",
              " 709: 'police',\n",
              " 710: 'brother',\n",
              " 711: 'it.',\n",
              " 712: 'roles',\n",
              " 713: 'especially',\n",
              " 714: 'service',\n",
              " 715: 'Switzerland.',\n",
              " 716: 'degree',\n",
              " 717: 'radio',\n",
              " 718: 'actor.',\n",
              " 719: '2015)',\n",
              " 720: 'right',\n",
              " 721: '19,',\n",
              " 722: 'together',\n",
              " 723: '2004',\n",
              " 724: 'months',\n",
              " 725: 'least',\n",
              " 726: '\"the',\n",
              " 727: 'followed',\n",
              " 728: 'research',\n",
              " 729: 'law',\n",
              " 730: 'raised',\n",
              " 731: 'Japan',\n",
              " 732: 'coast',\n",
              " 733: 'm',\n",
              " 734: 'While',\n",
              " 735: 'Europe',\n",
              " 736: 'Atlantic',\n",
              " 737: 'point',\n",
              " 738: 'traditional',\n",
              " 739: 'throughout',\n",
              " 740: 'once',\n",
              " 741: 'officially',\n",
              " 742: 'High',\n",
              " 743: 'year.',\n",
              " 744: 'Prize',\n",
              " 745: 'full',\n",
              " 746: 'leading',\n",
              " 747: 'replaced',\n",
              " 748: 'Peter',\n",
              " 749: '29',\n",
              " 750: '2018,',\n",
              " 751: 'brought',\n",
              " 752: 'election',\n",
              " 753: '=',\n",
              " 754: '31',\n",
              " 755: 'ten',\n",
              " 756: 'Joseph',\n",
              " 757: 'Australia.',\n",
              " 758: 'spent',\n",
              " 759: 'writer',\n",
              " 760: 'Championship',\n",
              " 761: 'mi',\n",
              " 762: 'whose',\n",
              " 763: '2014,',\n",
              " 764: 'Catholic',\n",
              " 765: 'Red',\n",
              " 766: 'changed',\n",
              " 767: 'ancient',\n",
              " 768: 'got',\n",
              " 769: 'result',\n",
              " 770: 'Southern',\n",
              " 771: 'administrative',\n",
              " 772: 'field',\n",
              " 773: 'win',\n",
              " 774: '2006,',\n",
              " 775: 'century.',\n",
              " 776: 'oldest',\n",
              " 777: 'California.',\n",
              " 778: 'mainly',\n",
              " 779: 'black',\n",
              " 780: 'run',\n",
              " 781: 'footballer',\n",
              " 782: 'uses',\n",
              " 783: 'stars',\n",
              " 784: 'air',\n",
              " 785: 'India',\n",
              " 786: 'study',\n",
              " 787: 'divided',\n",
              " 788: 'Parliament',\n",
              " 789: 'house',\n",
              " 790: 'science',\n",
              " 791: 'half',\n",
              " 792: 'Grand',\n",
              " 793: '2016)',\n",
              " 794: 'Latin',\n",
              " 795: 'Their',\n",
              " 796: '2001',\n",
              " 797: 'ended',\n",
              " 798: 'sea',\n",
              " 799: \"world's\",\n",
              " 800: 'food',\n",
              " 801: 'hit',\n",
              " 802: 'college',\n",
              " 803: 'becoming',\n",
              " 804: 'men',\n",
              " 805: 'California,',\n",
              " 806: 'Canada',\n",
              " 807: 'winning',\n",
              " 808: '50',\n",
              " 809: 'put',\n",
              " 810: 'Center',\n",
              " 811: '2014)',\n",
              " 812: 'church',\n",
              " 813: 'above',\n",
              " 814: 'himself',\n",
              " 815: 'Division',\n",
              " 816: '2013,',\n",
              " 817: 'gold',\n",
              " 818: 'remained',\n",
              " 819: 'ever',\n",
              " 820: 'war',\n",
              " 821: 'drama',\n",
              " 822: '2014.',\n",
              " 823: 'light',\n",
              " 824: '2015,',\n",
              " 825: 'novel',\n",
              " 826: 'social',\n",
              " 827: 'African',\n",
              " 828: 'women',\n",
              " 829: '2013.',\n",
              " 830: 'computer',\n",
              " 831: 'there.',\n",
              " 832: 'Black',\n",
              " 833: 'people.',\n",
              " 834: 'owned',\n",
              " 835: 'car',\n",
              " 836: '2012,',\n",
              " 837: 'able',\n",
              " 838: '2009,',\n",
              " 839: 'fifth',\n",
              " 840: 'change',\n",
              " 841: 'community',\n",
              " 842: 'competed',\n",
              " 843: 'must',\n",
              " 844: 'consists',\n",
              " 845: 'starring',\n",
              " 846: '2005,',\n",
              " 847: 'introduced',\n",
              " 848: 'special',\n",
              " 849: 'merged',\n",
              " 850: 'Island',\n",
              " 851: 'woman',\n",
              " 852: 'release',\n",
              " 853: 'Other',\n",
              " 854: 'average',\n",
              " 855: '2008,',\n",
              " 856: 'cities',\n",
              " 857: 'Association',\n",
              " 858: 'Sweden.',\n",
              " 859: 'independent',\n",
              " 860: 'widely',\n",
              " 861: 'far',\n",
              " 862: 'fire',\n",
              " 863: 'however,',\n",
              " 864: 'features',\n",
              " 865: 'areas',\n",
              " 866: 'native',\n",
              " 867: 'season.',\n",
              " 868: 'nominated',\n",
              " 869: 'reported',\n",
              " 870: 'Saint',\n",
              " 871: 'films',\n",
              " 872: 'La',\n",
              " 873: 'ran',\n",
              " 874: 'Prince',\n",
              " 875: 'rather',\n",
              " 876: 'approximately',\n",
              " 877: 'Battle',\n",
              " 878: 'Sweden',\n",
              " 879: 'road',\n",
              " 880: 'presidential',\n",
              " 881: 'away',\n",
              " 882: 'teams',\n",
              " 883: 'songs',\n",
              " 884: 'Queen',\n",
              " 885: 'division',\n",
              " 886: 'Brazilian',\n",
              " 887: 'contains',\n",
              " 888: 'shows',\n",
              " 889: 'Jr.',\n",
              " 890: 'sexual',\n",
              " 891: 'site',\n",
              " 892: 'Disney',\n",
              " 893: '2011.',\n",
              " 894: 'go',\n",
              " 895: 'start',\n",
              " 896: 'sent',\n",
              " 897: 'king',\n",
              " 898: 'types',\n",
              " 899: 'previously',\n",
              " 900: 'professor',\n",
              " 901: 'earned',\n",
              " 902: 'Senate',\n",
              " 903: 'area.',\n",
              " 904: 'Mary',\n",
              " 905: 'England,',\n",
              " 906: 'Nobel',\n",
              " 907: 'plant',\n",
              " 908: 'grew',\n",
              " 909: 'countries',\n",
              " 910: 'believed',\n",
              " 911: 'smaller',\n",
              " 912: 'beginning',\n",
              " 913: 'composed',\n",
              " 914: 'typically',\n",
              " 915: '2015.',\n",
              " 916: 'nine',\n",
              " 917: 'comes',\n",
              " 918: 'upon',\n",
              " 919: 'further',\n",
              " 920: 'added',\n",
              " 921: 'Angeles',\n",
              " 922: 'commercial',\n",
              " 923: '2007,',\n",
              " 924: 'primary',\n",
              " 925: 'Germany.',\n",
              " 926: 'Council',\n",
              " 927: 'met',\n",
              " 928: 'animated',\n",
              " 929: 'Korean',\n",
              " 930: 'child',\n",
              " 931: 'attended',\n",
              " 932: '(née',\n",
              " 933: 'low',\n",
              " 934: 'whom',\n",
              " 935: '20th',\n",
              " 936: 'passed',\n",
              " 937: 'level',\n",
              " 938: 'placed',\n",
              " 939: 'groups',\n",
              " 940: 'series.',\n",
              " 941: 'forms',\n",
              " 942: '(in',\n",
              " 943: 'cause',\n",
              " 944: 'estimated',\n",
              " 945: 'primarily',\n",
              " 946: 'artist',\n",
              " 947: 'good',\n",
              " 948: 'Kansas',\n",
              " 949: '2008.',\n",
              " 950: 'Airport',\n",
              " 951: 'writing',\n",
              " 952: 'eventually',\n",
              " 953: 'defeated',\n",
              " 954: 'To',\n",
              " 955: 'Empire',\n",
              " 956: 'addition',\n",
              " 957: 'sports',\n",
              " 958: 'Institute',\n",
              " 959: 'forces',\n",
              " 960: 'finished',\n",
              " 961: 'active',\n",
              " 962: 'airport',\n",
              " 963: 'numerous',\n",
              " 964: 'seasons',\n",
              " 965: 'related',\n",
              " 966: 'Act',\n",
              " 967: 'Games',\n",
              " 968: 'decided',\n",
              " 969: 'Winter',\n",
              " 970: '31,',\n",
              " 971: '2012.',\n",
              " 972: 'lower',\n",
              " 973: 'City,',\n",
              " 974: 'private',\n",
              " 975: 'century,',\n",
              " 976: 'Music',\n",
              " 977: 'Elizabeth',\n",
              " 978: 'year,',\n",
              " 979: 'London,',\n",
              " 980: 'saw',\n",
              " 981: '1994',\n",
              " 982: 'little',\n",
              " 983: 'parish',\n",
              " 984: 'Eastern',\n",
              " 985: 'should',\n",
              " 986: 'students',\n",
              " 987: 'metal',\n",
              " 988: 'business',\n",
              " 989: 'China',\n",
              " 990: 'author',\n",
              " 991: 'Born',\n",
              " 992: 'Department',\n",
              " 993: '),',\n",
              " 994: 'longer',\n",
              " 995: 'strong',\n",
              " 996: 'Before',\n",
              " 997: 'associated',\n",
              " 998: 'simply',\n",
              " 999: 'religious',\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.get_vocab_size('tokens')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruXQZ_EMLJDs",
        "outputId": "8c8d60c3-bf15-419a-c868-f7b1434b0bac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "165823"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # changed namespace to source_tokens\n",
        "# comp_embedding = Embedding(num_embeddings=vocab.get_vocab_size('source_tokens'),\n",
        "#                            embedding_dim=300,\n",
        "#                            vocab_namespace='source_tokens',\n",
        "#                            pretrained_file='/content/gdrive/MyDrive/playground/allennlp/data/glove.6B.300d.txt',\n",
        "#                            vocab=vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v47GZN8t2nXU",
        "outputId": "e06e892d-b86c-4472-dd8b-1225ee09901f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "400000it [00:03, 120393.47it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comp_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
        "                           embedding_dim=300,\n",
        "                           vocab_namespace='tokens',\n",
        "                           pretrained_file='/content/gdrive/MyDrive/playground/allennlp/data/glove.6B.300d.txt',\n",
        "                           vocab=vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqPgV7QIOyBr",
        "outputId": "6ca0a910-f288-40a5-fed2-ab74f9db9d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "400000it [00:07, 51752.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comp_embedding.num_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ngTdrv9LeB5",
        "outputId": "a04e5948-a36b-4f71-eadd-b51183c44e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "165823"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comp_embedding.output_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcdvkqODLmNq",
        "outputId": "2c6ad19e-a99d-48d0-f126-8a2bde071a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUvdrdaZnUio"
      },
      "outputs": [],
      "source": [
        "comp_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
        "                            embedding_dim=COMP_EMBEDDING_DIM,\n",
        "                            vocab_namespace='tokens')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comp_embedding.num_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bwsCSoGLz-S",
        "outputId": "f92919d2-30b2-42b7-ffd5-0831f3d2bb0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Here's how to save the model.\n",
        "# with open(\"/tmp/model.th\", 'wb') as f:\n",
        "#     torch.save(model.state_dict(), f)\n",
        "\n",
        "# vocab.save_to_files(\"/tmp/vocabulary\")\n",
        "\n",
        "# # And here's how to reload the model.\n",
        "# vocab2 = Vocabulary.from_files(\"/tmp/vocabulary\")\n",
        "\n",
        "# model2 = LstmTagger(word_embeddings, lstm, vocab2)\n",
        "# with open(\"/tmp/model.th\", 'rb') as f:\n",
        "#     model2.load_state_dict(torch.load(f))"
      ],
      "metadata": {
        "id": "hA7eNhDr8Z3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geYs4Fp9nUio",
        "outputId": "5fe63342-7f25-4e12-b805-1a04a2a9062f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "106442106\n"
          ]
        }
      ],
      "source": [
        "# encoder = StackedSelfAttentionEncoder(input_dim=COMP_EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, projection_dim=128, feedforward_hidden_dim=128,\n",
        "#                                         num_layers=1, num_attention_heads=8)\n",
        "\n",
        "# source_embedder = BasicTextFieldEmbedder({\"source_tokens\": comp_embedding})\n",
        "\n",
        "# attention = DotProductAttention()\n",
        "\n",
        "# max_decoding_steps = 20 #TODO: make this variable\n",
        "\n",
        "# # Define seq2seq model\n",
        "# model = SimpleSeq2Seq(vocab, source_embedder, encoder,\n",
        "#                         attention=attention,\n",
        "#                         target_namespace='target_tokens',\n",
        "#                         target_embedding_dim=SIMP_EMBEDDING_DIM,\n",
        "#                         use_bleu=True,\n",
        "#                         max_decoding_steps=max_decoding_steps)\n",
        "# model.to(device)\n",
        "# pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# print(pytorch_total_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = StackedSelfAttentionEncoder(input_dim=COMP_EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, projection_dim=128, feedforward_hidden_dim=128,\n",
        "                                        num_layers=1, num_attention_heads=4)\n",
        "\n",
        "source_embedder = BasicTextFieldEmbedder({\"tokens\": comp_embedding})\n",
        "\n",
        "attention = DotProductAttention()\n",
        "\n",
        "max_decoding_steps = 20 #TODO: make this variable\n",
        "\n",
        "# Define seq2seq model\n",
        "model = SimpleSeq2Seq(vocab, source_embedder, encoder,\n",
        "                        attention=attention,\n",
        "                        target_namespace='tokens',\n",
        "                        target_embedding_dim=SIMP_EMBEDDING_DIM,\n",
        "                        use_bleu=True,\n",
        "                        max_decoding_steps=max_decoding_steps)\n",
        "model.to(device)\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(pytorch_total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEQPoNU-O-H2",
        "outputId": "f940a4fd-3836-410e-e9e6-dfa8a8040719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "143148135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09WLmQOqnUip"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "train_loader.index_with(vocab)\n",
        "dev_loader.index_with(vocab)\n",
        "\n",
        "#modified num_epochs from 5 to 1\n",
        "trainer = GradientDescentTrainer(model=model,\n",
        "                serialization_dir='./saved_7',\n",
        "                data_loader=train_loader,\n",
        "                validation_data_loader=dev_loader,\n",
        "                num_epochs=2,\n",
        "                optimizer=optimizer\n",
        "                # validation_metric=\"+accuracy\"\n",
        "                )\n",
        "\n",
        "print(\"Starting training\")\n",
        "trainer.train()\n",
        "# for i in range(10):\n",
        "#   print('Epoch: {}'.format(i))\n",
        "#   trainer.train()\n",
        "print(\"Finished training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl_tuTSQ1M6b",
        "outputId": "3c22510b-7af0-4e7b-b21e-8767c944e565"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:allennlp.training.gradient_descent_trainer:You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
            "batch_loss: 4.6284, loss: 4.6284 ||:   0%|          | 1/7605 [00:00<17:44,  7.15it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch_loss: 4.9643, loss: 5.5860 ||: 100%|##########| 7605/7605 [13:09<00:00,  9.64it/s]\n",
            "BLEU: 0.0071, batch_loss: 5.5758, loss: 6.3680 ||: 100%|##########| 214/214 [00:28<00:00,  7.61it/s]\n",
            "batch_loss: 4.7123, loss: 4.7721 ||: 100%|##########| 7605/7605 [13:09<00:00,  9.64it/s]\n",
            "BLEU: 0.0072, batch_loss: 5.5376, loss: 6.4255 ||: 100%|##########| 214/214 [00:29<00:00,  7.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/gdrive/MyDrive/playground/allennlp/data/saved_6/best.th\", 'rb') as f:\n",
        "  model.load_state_dict(torch.load(f))"
      ],
      "metadata": {
        "id": "Fce6Syqk9h08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = Seq2SeqPredictor(model, reader)"
      ],
      "metadata": {
        "id": "-9V6VI_o5XR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsTmoEOxpWCU",
        "outputId": "13bcf312-d921-4fed-c715-235459167255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/playground/allennlp/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = reader.read('./subdata/validation_small1.tsv')"
      ],
      "metadata": {
        "id": "6RFMtAkM_xvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nw_iDqjnDi86",
        "outputId": "4600c442-f8f4-4cb4-8c7f-06213da24e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all.tsv\t\t    glove.6B.zip     simpleseq2seq.py\n",
            "composedseq2seq.py  saved\t     subdata\n",
            "create_bitext.py    saved_1\t     test.tsv\n",
            "dataset_readers     saved_2\t     train.tsv\n",
            "example.py\t    saved_3\t     Untitled.ipynb\n",
            "glove.6B.100d.txt   saved_4\t     validation-1.tsv\n",
            "glove.6B.200d.txt   saved_5\t     validation.tsv\n",
            "glove.6B.300d.txt   saved_6\t     wiki-auto-part-1-data.json\n",
            "glove.6B.50d.txt    seq2seq.jsonnet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for instance in itertools.islice(validation_dataset, 10):\n",
        "  print('SOURCE:', instance.fields['source_tokens'].tokens)\n",
        "  Complexity_labeller.convert_format_token(model_comp, instance.fields['source_tokens'].tokens)\n",
        "  dataframe = Complexity_labeller.get_dataframe(model_comp)\n",
        "  # list(zip(dataframe['sentences'].values[0],dataframe['labels'].values[0]))\n",
        "  print('COMPLEXITY:', list(zip(dataframe['sentences'].values[0],dataframe['labels'].values[0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID0URUZQDbc6",
        "outputId": "fc5b823b-2701-4539-f02f-1a77dfdb4d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SOURCE: [She, is, a, right, handed, batter.]\n",
            "COMPLEXITY: [('She', 0), ('is', 0), ('a', 0), ('right', 0), ('handed', 0), ('batter.', 1)]\n",
            "SOURCE: [Princess, Elisabeth, Charlotte, was, born, on, 27, May, 1652, in, Heidelberg, Castle,, to, Charles, I, Louis,, Elector, Palatine, of, the, Simmern, branch, of, the, House, of, Wittelsbach,, and, Landgravine, Charlotte, of, Hesse-Kassel.]\n",
            "COMPLEXITY: [('Princess', 1), ('Elisabeth', 1), ('Charlotte', 1), ('was', 0), ('born', 0), ('on', 0), ('27', 0), ('May', 0), ('1652', 0), ('in', 0), ('Heidelberg', 1), ('Castle,', 1), ('to', 0), ('Charles', 0), ('I', 0), ('Louis,', 0), ('Elector', 1), ('Palatine', 1), ('of', 0), ('the', 0), ('Simmern', 0), ('branch', 1), ('of', 0), ('the', 0), ('House', 0), ('of', 0), ('Wittelsbach,', 1), ('and', 0), ('Landgravine', 1), ('Charlotte', 1), ('of', 0), ('Hesse-Kassel.', 1)]\n",
            "SOURCE: [Roderick, James, Nugent, Stewart, (born, 3, January, 1973), is, a, British, Conservative, Party, politician, who, has, been, Member, of, Parliament, for, Penrith, and, The, Border, since, 2010.]\n",
            "COMPLEXITY: [('Roderick', 1), ('James', 0), ('Nugent', 0), ('Stewart', 0), ('(born', 1), ('3', 0), ('January', 0), ('1973)', 0), ('is', 0), ('a', 0), ('British', 0), ('Conservative', 1), ('Party', 0), ('politician', 1), ('who', 0), ('has', 0), ('been', 0), ('Member', 0), ('of', 0), ('Parliament', 1), ('for', 0), ('Penrith', 0), ('and', 0), ('The', 0), ('Border', 0), ('since', 0), ('2010.', 0)]\n",
            "SOURCE: [The, orchestra's, first, broadcast, was, on, November, 13,, 1937, and, it, continued, until, disbanded, in, 1954.]\n",
            "COMPLEXITY: [('The', 0), (\"orchestra's\", 0), ('first', 0), ('broadcast', 1), ('was', 0), ('on', 0), ('November', 0), ('13,', 0), ('1937', 0), ('and', 0), ('it', 0), ('continued', 1), ('until', 0), ('disbanded', 1), ('in', 0), ('1954.', 0)]\n",
            "SOURCE: [Michigan, is, the, fastest, track, in, NASCAR, due, to, its, wide,, sweeping, corners,, long, straightaways,, and, lack, of, a, restrictor, plate, requirement;, typical, qualifying, speeds, are, in, excess, of, 200, mph, and, corner, entry, speeds, are, anywhere, from, 215, to, after, the, 2012, repaving, of, the, track.]\n",
            "COMPLEXITY: [('Michigan', 1), ('is', 0), ('the', 0), ('fastest', 1), ('track', 0), ('in', 0), ('NASCAR', 0), ('due', 0), ('to', 0), ('its', 0), ('wide,', 1), ('sweeping', 1), ('corners,', 1), ('long', 0), ('straightaways,', 1), ('and', 0), ('lack', 1), ('of', 0), ('a', 0), ('restrictor', 1), ('plate', 0), ('requirement;', 1), ('typical', 1), ('qualifying', 1), ('speeds', 0), ('are', 0), ('in', 0), ('excess', 1), ('of', 0), ('200', 0), ('mph', 1), ('and', 0), ('corner', 1), ('entry', 1), ('speeds', 0), ('are', 0), ('anywhere', 1), ('from', 0), ('215', 0), ('to', 0), ('after', 0), ('the', 0), ('2012', 0), ('repaving', 1), ('of', 0), ('the', 0), ('track.', 1)]\n",
            "SOURCE: [The, UNESCO, committee, stated, the, place, was, an, \"outstanding, example, of, Far, Eastern, palace, architecture, and, garden, design\", being, exceptional, because, the, buildings, are, \"integrated, into, and, harmonized, with, the, natural, setting\", and, adapted, \"to, the, topography, and, retaining, indigenous, tree, cover.\"]\n",
            "COMPLEXITY: [('The', 0), ('UNESCO', 1), ('committee', 0), ('stated', 1), ('the', 0), ('place', 0), ('was', 0), ('an', 0), ('\"outstanding', 1), ('example', 0), ('of', 0), ('Far', 0), ('Eastern', 0), ('palace', 0), ('architecture', 1), ('and', 0), ('garden', 0), ('design\"', 1), ('being', 0), ('exceptional', 1), ('because', 0), ('the', 0), ('buildings', 0), ('are', 0), ('\"integrated', 1), ('into', 0), ('and', 0), ('harmonized', 1), ('with', 0), ('the', 0), ('natural', 0), ('setting\"', 1), ('and', 0), ('adapted', 1), ('\"to', 1), ('the', 0), ('topography', 1), ('and', 0), ('retaining', 1), ('indigenous', 1), ('tree', 0), ('cover.\"', 0)]\n",
            "SOURCE: [He, played, for, the, Syracuse, Nationals, /, Philadelphia, 76ers, of, the, National, Basketball, Association, (NBA), from, 1958, through, 1973.]\n",
            "COMPLEXITY: [('He', 0), ('played', 0), ('for', 0), ('the', 0), ('Syracuse', 1), ('Nationals', 0), ('/', 1), ('Philadelphia', 1), ('76ers', 0), ('of', 0), ('the', 0), ('National', 0), ('Basketball', 1), ('Association', 0), ('(NBA)', 0), ('from', 0), ('1958', 0), ('through', 0), ('1973.', 0)]\n",
            "SOURCE: [He, lived, in, London, for, 14, years,, where, he, worked, for, the, British, Broadcasting, Corporation, and, in, France,, where, he, worked, for, Radio, France, Internationale, and, briefly, served, as, Consul, General, of, Mexico.]\n",
            "COMPLEXITY: [('He', 0), ('lived', 0), ('in', 0), ('London', 0), ('for', 0), ('14', 0), ('years,', 1), ('where', 0), ('he', 0), ('worked', 0), ('for', 0), ('the', 0), ('British', 0), ('Broadcasting', 0), ('Corporation', 1), ('and', 0), ('in', 0), ('France,', 1), ('where', 0), ('he', 0), ('worked', 0), ('for', 0), ('Radio', 0), ('France', 0), ('Internationale', 1), ('and', 0), ('briefly', 1), ('served', 0), ('as', 0), ('Consul', 1), ('General', 0), ('of', 0), ('Mexico.', 0)]\n",
            "SOURCE: [It, is, one, of, the, largest, towns, in, the, Federally, Administered, Tribal, Areas,, and, is, located, 1072, m, above, sea, level,, on, the, route, across, the, mountains, to, the, city, of, Peshawar,, Khyber, Pakhtunkhwa,, Pakistan.]\n",
            "COMPLEXITY: [('It', 0), ('is', 0), ('one', 0), ('of', 0), ('the', 0), ('largest', 0), ('towns', 0), ('in', 0), ('the', 0), ('Federally', 1), ('Administered', 1), ('Tribal', 1), ('Areas,', 1), ('and', 0), ('is', 0), ('located', 0), ('1072', 0), ('m', 0), ('above', 0), ('sea', 0), ('level,', 1), ('on', 0), ('the', 0), ('route', 1), ('across', 0), ('the', 0), ('mountains', 0), ('to', 0), ('the', 0), ('city', 0), ('of', 0), ('Peshawar,', 1), ('Khyber', 1), ('Pakhtunkhwa,', 0), ('Pakistan.', 0)]\n",
            "SOURCE: [Sibley, wrote, his, classic, field, guide,, \"The, Sibley, Guide, to, Birds\",, while, he, was, living, and, birding, in, Cape, May, Point.]\n",
            "COMPLEXITY: [('Sibley', 0), ('wrote', 0), ('his', 0), ('classic', 1), ('field', 0), ('guide,', 0), ('\"The', 1), ('Sibley', 0), ('Guide', 0), ('to', 0), ('Birds\",', 1), ('while', 0), ('he', 0), ('was', 0), ('living', 0), ('and', 0), ('birding', 1), ('in', 0), ('Cape', 0), ('May', 0), ('Point.', 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for instance in itertools.islice(validation_dataset, 10):\n",
        "  print('SOURCE:', instance.fields['source_tokens'].tokens)\n",
        "  print('GOLD:', instance.fields['target_tokens'].tokens)\n",
        "  print('PRED:', predictor.predict_instance(instance)['predicted_tokens'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBB-CK74Dqw8",
        "outputId": "6dd9c46b-17ca-4f6b-85ef-304c88149258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SOURCE: [She, is, a, right, handed, batter.]\n",
            "GOLD: [@start@, She, is, a, right, handed, bat., @end@]\n",
            "PRED: [['He', 'is', 'a', 'member', 'of', 'the', 'Democratic', 'Party.'], ['He', 'is', 'a', 'member', 'of', 'the', 'United', 'States.'], ['He', 'is', 'a', 'member', 'of', 'the', 'U.S.', 'state', 'of', 'Fame.'], ['He', 'is', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives.'], ['He', 'is', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'is', 'a', 'member', 'of', 'the', 'U.S.', 'state', 'of', 'South', 'Dakota.'], ['He', 'is', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives'], ['He', 'is', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States.'], ['He', 'is', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', '2010', 'census.'], ['He', 'is', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'U.S.', 'state', 'of', 'South', 'Dakota.']]\n",
            "SOURCE: [Princess, Elisabeth, Charlotte, was, born, on, 27, May, 1652, in, Heidelberg, Castle,, to, Charles, I, Louis,, Elector, Palatine, of, the, Simmern, branch, of, the, House, of, Wittelsbach,, and, Landgravine, Charlotte, of, Hesse-Kassel.]\n",
            "GOLD: [@start@, Elisabeth, Charlotte, was, born, at, the, Heidelberg, Castle, as, the, only, daughter, of, Charles, I, Louis,, Elector, Palatine, and, Charlotte, of, Hesse-Kassel., @end@]\n",
            "PRED: [['He', 'was', 'a', 'member', 'of', 'the', 'Democratic', 'Party.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'Ambassador', 'to', 'the', 'United', 'States.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'Ambassador', 'to', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'Ambassador', 'to', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States.']]\n",
            "SOURCE: [Roderick, James, Nugent, Stewart, (born, 3, January, 1973), is, a, British, Conservative, Party, politician, who, has, been, Member, of, Parliament, for, Penrith, and, The, Border, since, 2010.]\n",
            "GOLD: [@start@, Roderick, James, Nugent, \"Rory\", Stewart, (born, 3, January, 1973), is, a, British, Conservative, Party, politician., @end@]\n",
            "PRED: [['He', 'is', 'a', 'member', 'of', 'the', 'Democratic', 'Party.'], ['He', 'is', 'a', 'member', 'of', 'the', 'National', 'Football', 'League', '(NHL).'], ['He', 'is', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'is', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives.'], ['She', 'is', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'is', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States.'], ['She', 'is', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States.'], ['He', 'is', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'is', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States', 'House', 'of', 'Representatives.'], ['He', 'is', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the']]\n",
            "SOURCE: [The, orchestra's, first, broadcast, was, on, November, 13,, 1937, and, it, continued, until, disbanded, in, 1954.]\n",
            "GOLD: [@start@, It, began, November, 13,, 1937., @end@]\n",
            "PRED: [['He', 'was', 'born', 'in', 'Los', 'Angeles,', 'California.'], ['He', 'was', 'born', 'in', 'Paris.'], ['He', 'was', 'born', 'in', 'New', 'York', 'City.'], ['He', 'was', 'born', 'in', 'New', 'York', 'City,', 'California.'], ['He', 'was', 'born', 'in', 'New', 'York', 'City', 'on', 'December', '25,', '2013.'], ['He', 'was', 'born', 'in', 'New', 'York', 'City', 'on', 'December', '19,', '2013.'], ['He', 'was', 'born', 'in', 'New', 'York', 'City', 'on', 'December', '25,', '2014.'], ['He', 'was', 'born', 'in', 'New', 'York', 'City', 'on', 'March', '25,', '2013.'], ['He', 'was', 'born', 'in', 'New', 'York', 'City', 'on', 'December', '19,', '2014.'], ['He', 'was', 'born', 'in', 'New', 'York', 'City', 'on', 'December', '25,', '2019,', 'at', 'the', 'age', 'of', '85.']]\n",
            "SOURCE: [Michigan, is, the, fastest, track, in, NASCAR, due, to, its, wide,, sweeping, corners,, long, straightaways,, and, lack, of, a, restrictor, plate, requirement;, typical, qualifying, speeds, are, in, excess, of, 200, mph, and, corner, entry, speeds, are, anywhere, from, 215, to, after, the, 2012, repaving, of, the, track.]\n",
            "GOLD: [@start@, Michigan, is, now, one, of, the, fastest, tracks, in, NASCAR, due, to, its, wide,, sweeping, corners, and, long, straightaways., @end@]\n",
            "PRED: [['He', 'was', 'born', 'in', 'Paris.'], ['He', 'was', 'born', 'in', 'Los', 'Angeles,', 'California.'], ['He', 'was', 'born', 'in', 'New', 'York', 'City.'], ['It', 'is', 'the', 'largest', 'city', 'in', 'the', 'United', 'States.'], ['It', 'is', 'one', 'of', 'the', 'most', 'popular', 'in', 'the', 'world.'], ['It', 'is', 'one', 'of', 'the', 'most', 'popular', 'in', 'the', 'United', 'States.'], ['It', 'is', 'the', 'largest', 'city', 'in', 'the', 'United', 'States', 'in', 'the', 'United', 'States.'], ['It', 'is', 'one', 'of', 'the', 'most', 'popular', 'in', 'the', 'United', 'States', 'in', 'the', 'world.'], ['It', 'is', 'one', 'of', 'the', 'most', 'popular', 'in', 'the', 'United', 'States', 'in', 'the', 'United', 'States.'], ['It', 'is', 'one', 'of', 'the', 'most', 'popular', 'in', 'the', 'United', 'States', 'in', 'the', 'United', 'States', 'in', 'the', 'United', 'States.']]\n",
            "SOURCE: [The, UNESCO, committee, stated, the, place, was, an, \"outstanding, example, of, Far, Eastern, palace, architecture, and, garden, design\", being, exceptional, because, the, buildings, are, \"integrated, into, and, harmonized, with, the, natural, setting\", and, adapted, \"to, the, topography, and, retaining, indigenous, tree, cover.\"]\n",
            "GOLD: [@start@, The, UNESCO, said, that, Changdukgung, 'was, outstanding, example, of, Far, Eastern, palace, architecture, and, garden, design'., @end@]\n",
            "PRED: [['He', 'was', 'a', 'member', 'of', 'the', 'Democratic', 'Party.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'Ambassador', 'to', 'the', 'United', 'States.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'Ambassador', 'to', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States', 'House', 'of', 'Representatives.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'Ambassador', 'to', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States.']]\n",
            "SOURCE: [He, played, for, the, Syracuse, Nationals, /, Philadelphia, 76ers, of, the, National, Basketball, Association, (NBA), from, 1958, through, 1973.]\n",
            "GOLD: [@start@, He, played, for, the, Syracuse, Nationals, /, Philadelphia, 76ers, of, the, National, Basketball, Association, (NBA), from, 1958, through, 1973., @end@]\n",
            "PRED: [['He', 'has', 'played', 'for', 'the', 'Japanese', 'national', 'team.'], ['He', 'was', 'a', 'member', 'of', 'the', 'Democratic', 'Party.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'Ambassador', 'to', 'the', 'United', 'States.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'Ambassador', 'to', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'Ambassador', 'to', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'Ambassador', 'to', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'University', 'of'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'Ambassador', 'to', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States']]\n",
            "SOURCE: [He, lived, in, London, for, 14, years,, where, he, worked, for, the, British, Broadcasting, Corporation, and, in, France,, where, he, worked, for, Radio, France, Internationale, and, briefly, served, as, Consul, General, of, Mexico.]\n",
            "GOLD: [@start@, He, lived, in, London, for, 14, years., @end@]\n",
            "PRED: [['He', 'was', 'born', 'in', 'Los', 'Angeles,', 'California.'], ['He', 'was', 'born', 'in', 'Paris.'], ['He', 'was', 'born', 'in', 'Sydney.'], ['He', 'was', 'born', 'in', 'New', 'York', 'City.'], ['He', 'was', 'born', 'in', 'Brooklyn,', 'New', 'York.'], ['She', 'was', 'born', 'in', 'New', 'York', 'City.'], ['He', 'was', 'born', 'in', 'New', 'South', 'Wales.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the']]\n",
            "SOURCE: [It, is, one, of, the, largest, towns, in, the, Federally, Administered, Tribal, Areas,, and, is, located, 1072, m, above, sea, level,, on, the, route, across, the, mountains, to, the, city, of, Peshawar,, Khyber, Pakhtunkhwa,, Pakistan.]\n",
            "GOLD: [@start@, It, is, on, a, route, across, the, mountains, from, the, near-border, city,, Peshawar., @end@]\n",
            "PRED: [['It', 'is', 'one', 'of', 'the', 'most', 'popular', 'in', 'the', 'United', 'States.'], ['He', 'is', 'best', 'known', 'for', 'his', 'role', 'in', 'the', 'United', 'States.'], ['He', 'is', 'best', 'known', 'for', 'his', 'roles', 'in', 'the', 'United', 'States.'], ['He', 'is', 'best', 'known', 'for', 'his', 'roles', 'in', 'the', 'National', 'Hockey', 'League', '(NHL).'], ['He', 'is', 'best', 'known', 'for', 'his', 'roles', 'in', 'the', 'National', 'Hockey', 'League', '(NFL).'], ['He', 'is', 'best', 'known', 'for', 'his', 'role', 'in', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'is', 'best', 'known', 'for', 'his', 'roles', 'in', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'is', 'best', 'known', 'for', 'his', 'work', 'in', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'is', 'best', 'known', 'for', 'his', 'role', 'in', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States.'], ['He', 'is', 'best', 'known', 'for', 'his', 'roles', 'in', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States.']]\n",
            "SOURCE: [Sibley, wrote, his, classic, field, guide,, \"The, Sibley, Guide, to, Birds\",, while, he, was, living, and, birding, in, Cape, May, Point.]\n",
            "GOLD: [@start@, Sibley, wrote, the, field, guide, \"The, Sibley, Guide, to, Birds\"., @end@]\n",
            "PRED: [['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'December', '2013.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', '1965', 'to', '2013.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', '1989', 'to', '2013.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States', 'House', 'of', 'Representatives.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States', 'House', 'of', 'Fame.'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'December'], ['He', 'was', 'a', 'member', 'of', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the', 'United', 'States', 'House', 'of', 'Representatives', 'from', 'the']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5pzif77nUiq"
      },
      "outputs": [],
      "source": [
        "%pip install GPUtil\n",
        "\n",
        "from GPUtil import showUtilization as gpu_usage\n",
        "gpu_usage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kvgDLs5nUiq"
      },
      "outputs": [],
      "source": [
        "# for i in range(50):\n",
        "#     print('Epoch: {}'.format(i))\n",
        "#     trainer.train()\n",
        "\n",
        "#     predictor = Seq2SeqPredictor(model, reader)\n",
        "#     for instance in itertools.islice(validation_dataset, 10):\n",
        "#         print('SOURCE:', instance.fields['source_tokens'].tokens)\n",
        "#         print('GOLD:', instance.fields['target_tokens'].tokens)\n",
        "#         print('PRED:', predictor.predict_instance(instance)['predicted_tokens'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXGI_b17nUir",
        "outputId": "cebe1ac8-e20a-4385-b5fc-5ee540d94566"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.current_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugCS5LBQnUis"
      },
      "outputs": [],
      "source": [
        "%cat all.tsv | awk 'NR%10!=1&&NR%10!=2' > train.tsv"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "21c5b1d91b7ee749994cf4cdadcde87ec7043048ab4e5d5fad50d8cb91a2582b"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "collapsed_sections": [
        "78oU5J_LWZbL"
      ]
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}